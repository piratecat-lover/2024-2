{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trends Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "# Load credits.csv\n",
    "credits = pd.read_csv('./raw-data/credits.csv')\n",
    "\n",
    "# Load keywords.csv\n",
    "keywords = pd.read_csv('./raw-data/keywords.csv')\n",
    "\n",
    "# Load links.csv\n",
    "links = pd.read_csv('./raw-data/links.csv')\n",
    "\n",
    "# Load movies_metadata.csv\n",
    "movies_metadata = pd.read_csv('./raw-data/movies_metadata.csv', low_memory=False)\n",
    "\n",
    "# Load ratings.csv\n",
    "ratings = pd.read_csv('./raw-data/ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON to Pandas DataFrames\n",
    "\n",
    "# Function to safely parse JSON-like strings\n",
    "def parse_json_column(df, column_name):\n",
    "    def parse_json(x):\n",
    "        if pd.isna(x):\n",
    "            return []\n",
    "        try:\n",
    "            return ast.literal_eval(x)\n",
    "        except:\n",
    "            return []\n",
    "    df[column_name] = df[column_name].apply(parse_json)\n",
    "    return df\n",
    "\n",
    "# Function to flatten list of dictionaries into a DataFrame\n",
    "def flatten_column(df, column_name):\n",
    "    flattened_data = df[column_name].apply(pd.Series).stack().reset_index(level=1, drop=True)\n",
    "    flattened_df = pd.DataFrame(list(flattened_data))\n",
    "    return flattened_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse 'cast' and 'crew' in credits\n",
    "credits = parse_json_column(credits, 'cast')\n",
    "credits = parse_json_column(credits, 'crew')\n",
    "\n",
    "# Parse 'keywords' in keywords\n",
    "keywords = parse_json_column(keywords, 'keywords')\n",
    "\n",
    "# Parse relevant columns in movies_metadata\n",
    "json_columns = ['genres', 'production_companies', 'production_countries', 'spoken_languages', 'belongs_to_collection']\n",
    "for column in json_columns:\n",
    "    movies_metadata = parse_json_column(movies_metadata, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_actors(cast_list, n=3):\n",
    "    \"\"\"Extract top N actor names from the cast list.\"\"\"\n",
    "    if isinstance(cast_list, list):\n",
    "        # Sort cast by 'order' to get the billing order\n",
    "        cast_list = sorted(cast_list, key=lambda x: x.get('order', 999))\n",
    "        actors = [member.get('name') for member in cast_list[:n] if member.get('name')]\n",
    "        return actors\n",
    "    return []\n",
    "\n",
    "credits['main_actors'] = credits['cast'].apply(get_top_actors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_director(crew_list):\n",
    "    \"\"\"Extract the director's name from the crew list.\"\"\"\n",
    "    if isinstance(crew_list, list):\n",
    "        for member in crew_list:\n",
    "            if member.get('job') == 'Director':\n",
    "                return member.get('name')\n",
    "    return np.nan\n",
    "\n",
    "credits['director'] = credits['crew'].apply(get_director)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(keywords_list):\n",
    "    \"\"\"Extract keyword names.\"\"\"\n",
    "    return [kw.get('name') for kw in keywords_list if kw.get('name')]\n",
    "\n",
    "keywords['keyword_list'] = keywords['keywords'].apply(get_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Extract country codes from production_countries.\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [country\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miso_3166_1\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m country \u001b[38;5;129;01min\u001b[39;00m production_countries_list \u001b[38;5;28;01mif\u001b[39;00m country\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miso_3166_1\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m----> 5\u001b[0m movies_metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduction_countries_list\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mmovies_metadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproduction_countries\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_production_countries\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Max-PC\\.conda\\envs\\bir_env\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Max-PC\\.conda\\envs\\bir_env\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Max-PC\\.conda\\envs\\bir_env\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\Max-PC\\.conda\\envs\\bir_env\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Max-PC\\.conda\\envs\\bir_env\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m, in \u001b[0;36mget_production_countries\u001b[1;34m(production_countries_list)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_production_countries\u001b[39m(production_countries_list):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Extract country codes from production_countries.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mcountry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miso_3166_1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcountry\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduction_countries_list\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcountry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miso_3166_1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "def get_production_countries(production_countries_list):\n",
    "    \"\"\"Extract country codes from production_countries.\"\"\"\n",
    "    return [country.get('iso_3166_1') for country in production_countries_list if country.get('iso_3166_1')]\n",
    "\n",
    "movies_metadata['production_countries_list'] = movies_metadata['production_countries'].apply(get_production_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_metadata['original_language'] = movies_metadata['original_language'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_collection(collection):\n",
    "    \"\"\"Determine if a movie is part of a collection.\"\"\"\n",
    "    return 1 if isinstance(collection, dict) else 0\n",
    "\n",
    "movies_metadata['has_collection'] = movies_metadata['belongs_to_collection'].apply(has_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'release_date' to datetime\n",
    "movies_metadata['release_date'] = pd.to_datetime(movies_metadata['release_date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'id', 'budget', 'revenue', 'popularity' to numeric\n",
    "movies_metadata['id'] = pd.to_numeric(movies_metadata['id'], errors='coerce')\n",
    "movies_metadata['budget'] = pd.to_numeric(movies_metadata['budget'], errors='coerce')\n",
    "movies_metadata['revenue'] = pd.to_numeric(movies_metadata['revenue'], errors='coerce')\n",
    "movies_metadata['popularity'] = pd.to_numeric(movies_metadata['popularity'], errors='coerce')\n",
    "movies_metadata['vote_average'] = pd.to_numeric(movies_metadata['vote_average'], errors='coerce')\n",
    "movies_metadata['vote_count'] = pd.to_numeric(movies_metadata['vote_count'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'movieId' in ratings and 'tmdbId' in links to numeric\n",
    "ratings['movieId'] = pd.to_numeric(ratings['movieId'], errors='coerce')\n",
    "links['movieId'] = pd.to_numeric(links['movieId'], errors='coerce')\n",
    "links['tmdbId'] = pd.to_numeric(links['tmdbId'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge ratings and links on 'movieId'\n",
    "ratings_links = ratings.merge(links[['movieId', 'tmdbId']], on='movieId', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'tmdbId' to compute average ratings and count of ratings\n",
    "average_ratings = ratings_links.groupby('tmdbId')['rating'].agg(['mean', 'count']).reset_index()\n",
    "average_ratings.rename(columns={'mean': 'average_rating', 'count': 'rating_count'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove entries with missing 'tmdbId'\n",
    "average_ratings = average_ratings[average_ratings['tmdbId'].notnull()]\n",
    "average_ratings['tmdbId'] = average_ratings['tmdbId'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'id' columns are of the same type\n",
    "credits['id'] = pd.to_numeric(credits['id'], errors='coerce')\n",
    "keywords['id'] = pd.to_numeric(keywords['id'], errors='coerce')\n",
    "\n",
    "# Merge credits and keywords with movies_metadata\n",
    "df = movies_metadata.merge(credits[['id', 'main_actors', 'director']], on='id', how='left')\n",
    "df = df.merge(keywords[['id', 'keyword_list']], on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge average ratings with df on 'id' and 'tmdbId'\n",
    "df = df.merge(average_ratings, left_on='id', right_on='tmdbId', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant features\n",
    "df = df[['id', 'title', 'release_date', 'main_actors', 'director', 'keyword_list',\n",
    "         'production_countries_list', 'original_language', 'has_collection',\n",
    "         'budget', 'revenue', 'popularity', 'average_rating', 'rating_count', 'vote_average', 'vote_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing release dates\n",
    "df = df.dropna(subset=['release_date'])\n",
    "\n",
    "# Fill missing values in lists with empty lists\n",
    "df['main_actors'] = df['main_actors'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "df['keyword_list'] = df['keyword_list'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "df['production_countries_list'] = df['production_countries_list'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "# Fill missing numerical values with 0\n",
    "numerical_columns = ['budget', 'revenue', 'popularity', 'average_rating', 'rating_count', 'vote_average', 'vote_count']\n",
    "df[numerical_columns] = df[numerical_columns].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid division by zero\n",
    "df['budget'] = df['budget'].replace(0, np.nan)\n",
    "df['revenue_budget_ratio'] = df['revenue'] / df['budget']\n",
    "df['revenue_budget_ratio'] = df['revenue_budget_ratio'].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "df['budget'] = df['budget'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'average_rating' and 'vote_average'\n",
    "df['final_rating'] = df.apply(lambda x: x['average_rating'] if x['average_rating'] > 0 else x['vote_average'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by='release_date').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizers for different features\n",
    "\n",
    "# Initialize vocabularies\n",
    "actor_vocab = {}\n",
    "director_vocab = {}\n",
    "keyword_vocab = {}\n",
    "country_vocab = {}\n",
    "language_vocab = {}\n",
    "\n",
    "# Build actor vocabulary\n",
    "actor_counter = 1  # Starting index\n",
    "for actors in df['main_actors']:\n",
    "    for actor in actors:\n",
    "        if actor not in actor_vocab:\n",
    "            actor_vocab[actor] = actor_counter\n",
    "            actor_counter += 1\n",
    "            \n",
    "# Build director vocabulary\n",
    "director_counter = 1\n",
    "for director in df['director']:\n",
    "    if director not in director_vocab:\n",
    "        director_vocab[director] = director_counter\n",
    "        director_counter += 1\n",
    "\n",
    "# Build keyword vocabulary\n",
    "keyword_counter = 1\n",
    "for keywords in df['keyword_list']:\n",
    "    for keyword in keywords:\n",
    "        if keyword not in keyword_vocab:\n",
    "            keyword_vocab[keyword] = keyword_counter\n",
    "            keyword_counter += 1\n",
    "            \n",
    "# Build country vocabulary\n",
    "country_counter = 1\n",
    "for countries in df['production_countries_list']:\n",
    "    for country in countries:\n",
    "        if country not in country_vocab:\n",
    "            country_vocab[country] = country_counter\n",
    "            country_counter += 1\n",
    "            \n",
    "# Build language vocabulary\n",
    "language_counter = 1\n",
    "for lang in df['original_language']:\n",
    "    if lang not in language_vocab:\n",
    "        language_vocab[lang] = language_counter\n",
    "        language_counter += 1\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map actors to sequences\n",
    "df['actor_seq'] = df['main_actors'].apply(lambda actors: [actor_vocab[actor] for actor in actors])\n",
    "\n",
    "# Map director to sequence\n",
    "df['director_seq'] = df['director'].apply(lambda director: [director_vocab[director]])\n",
    "\n",
    "# Map keywords to sequences\n",
    "df['keyword_seq'] = df['keyword_list'].apply(lambda keywords: [keyword_vocab[keyword] for keyword in keywords])\n",
    "\n",
    "\n",
    "# Map countries to sequences\n",
    "df['country_seq'] = df['production_countries_list'].apply(lambda countries: [country_vocab[country] for country in countries])\n",
    "\n",
    "\n",
    "# Map language to sequence\n",
    "df['language_seq'] = df['original_language'].apply(lambda lang: [language_vocab[lang]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(seq, maxlen):\n",
    "    seq = seq[:maxlen]\n",
    "    seq = seq + [0] * (maxlen - len(seq))\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define maximum lengths for each sequence\n",
    "max_actor_length = 3  # Since we have top 3 actors\n",
    "max_director_length = 1\n",
    "max_keyword_length = 10  # Adjust based on data\n",
    "max_country_length = 3\n",
    "max_language_length = 1\n",
    "\n",
    "# Pad actor sequences\n",
    "df['actor_seq_padded'] = df['actor_seq'].apply(lambda seq: pad_sequence(seq, max_actor_length))\n",
    "\n",
    "# Pad director sequences\n",
    "df['director_seq_padded'] = df['director_seq'].apply(lambda seq: pad_sequence(seq, max_director_length))\n",
    "\n",
    "# Pad keyword sequences\n",
    "df['keyword_seq_padded'] = df['keyword_seq'].apply(lambda seq: pad_sequence(seq, max_keyword_length))\n",
    "\n",
    "# Pad country sequences\n",
    "df['country_seq_padded'] = df['country_seq'].apply(lambda seq: pad_sequence(seq, max_country_length))\n",
    "\n",
    "# Pad language sequences\n",
    "df['language_seq_padded'] = df['language_seq'].apply(lambda seq: pad_sequence(seq, max_language_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sequences(row):\n",
    "    sequences = []\n",
    "    sequences.extend(row['actor_seq_padded'])\n",
    "    sequences.extend(row['director_seq_padded'])\n",
    "    sequences.extend(row['keyword_seq_padded'])\n",
    "    sequences.extend(row['country_seq_padded'])\n",
    "    sequences.extend(row['language_seq_padded'])\n",
    "    sequences.append(row['has_collection'])\n",
    "    return sequences\n",
    "\n",
    "df['input_sequence'] = df.apply(combine_sequences, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert input sequences to NumPy array\n",
    "X = np.array(df['input_sequence'].tolist())\n",
    "\n",
    "# Extract target variables\n",
    "y_revenue = df['revenue'].values\n",
    "y_rev_budget_ratio = df['revenue_budget_ratio'].values\n",
    "y_rating = df['final_rating'].values\n",
    "y_popularity = df['popularity'].values\n",
    "\n",
    "# Combine target variables into a single array\n",
    "y_targets = np.vstack((y_revenue, y_rev_budget_ratio, y_rating, y_popularity)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale target variables using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "y_targets_scaled = scaler.fit_transform(y_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weights for each metric (adjust as needed)\n",
    "weights = np.array([0.4, 0.3, 0.2, 0.1])\n",
    "\n",
    "# Compute the combined target variable\n",
    "combined_target = y_targets_scaled @ weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append combined target as the last column\n",
    "y_targets_combined = np.hstack((y_targets_scaled, combined_target.reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.LongTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_targets_combined, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MovieDataset(X_train, y_train)\n",
    "test_dataset = MovieDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers=1):\n",
    "        super(MovieLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        # Assuming sequence length is small, we can get the last output\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        # Use the last output of the LSTM\n",
    "        out = lstm_out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# Calculate total vocabulary size\n",
    "vocab_size = max(\n",
    "    max(actor_vocab.values(), default=0),\n",
    "    max(director_vocab.values(), default=0),\n",
    "    max(keyword_vocab.values(), default=0),\n",
    "    max(country_vocab.values(), default=0),\n",
    "    max(language_vocab.values(), default=0)\n",
    ") + 1  # Plus one for padding (index 0)\n",
    "\n",
    "embedding_dim = 128\n",
    "hidden_dim = 64\n",
    "output_dim = y_targets_combined.shape[1]  # Number of target variables (including combined target)\n",
    "\n",
    "model = MovieLSTM(vocab_size, embedding_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    test_loss /= len(test_dataset)\n",
    "    print(f'Test Loss: {test_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bir_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
