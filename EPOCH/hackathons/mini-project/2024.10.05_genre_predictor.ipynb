{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"./raw-data/movies_metadata.csv\", low_memory=False)\n",
    "ratings_df = pd.read_csv(\"./ratings_average.csv\")\n",
    "\n",
    "# 1. Cleaning Genre Data\n",
    "def parse_genres(genre_str):\n",
    "    try:\n",
    "        genres = ast.literal_eval(genre_str)\n",
    "        if isinstance(genres, list) and len(genres) > 0:\n",
    "            return [genre['name'] for genre in genres if 'name' in genre]\n",
    "    except (ValueError, SyntaxError):\n",
    "        pass\n",
    "    return []\n",
    "\n",
    "df['parsed_genres'] = df['genres'].apply(parse_genres)\n",
    "\n",
    "# 2. Cleaning Budget and Revenue\n",
    "def clean_numeric(value):\n",
    "    try:\n",
    "        val = int(value)\n",
    "        if val == 0:\n",
    "            return np.nan  # Treat 0 as missing value\n",
    "        return val\n",
    "    except (ValueError, TypeError):\n",
    "        return np.nan\n",
    "\n",
    "df['budget_cleaned'] = df['budget'].apply(clean_numeric)\n",
    "df['revenue_cleaned'] = df['revenue'].apply(clean_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max-PC\\AppData\\Local\\Temp\\ipykernel_26528\\3283686803.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['release_month'] = df_cleaned['release_date'].dt.month\n",
      "C:\\Users\\Max-PC\\AppData\\Local\\Temp\\ipykernel_26528\\3283686803.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['release_day'] = df_cleaned['release_date'].dt.day\n",
      "C:\\Users\\Max-PC\\AppData\\Local\\Temp\\ipykernel_26528\\3283686803.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['revenue_budget_ratio'] = df_cleaned['revenue_cleaned'] / df_cleaned['budget_cleaned']\n",
      "C:\\Users\\Max-PC\\AppData\\Local\\Temp\\ipykernel_26528\\3283686803.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['id'] = pd.to_numeric(df_cleaned['id'], errors='coerce')  # Ensure 'id' is numeric to match 'movieId'\n"
     ]
    }
   ],
   "source": [
    "# 3. Cleaning Release Date\n",
    "df['release_date'] = pd.to_datetime(df['release_date'], errors='coerce')\n",
    "\n",
    "# Drop rows where budget or revenue is NaN or release date is missing\n",
    "df_cleaned = df.dropna(subset=['budget_cleaned', 'revenue_cleaned', 'release_date'])\n",
    "\n",
    "# Extract month and day from release date\n",
    "df_cleaned['release_month'] = df_cleaned['release_date'].dt.month\n",
    "df_cleaned['release_day'] = df_cleaned['release_date'].dt.day\n",
    "\n",
    "# Step 5: Target variable (revenue/budget ratio)\n",
    "df_cleaned['revenue_budget_ratio'] = df_cleaned['revenue_cleaned'] / df_cleaned['budget_cleaned']\n",
    "\n",
    "# Step 6: Add ratings data\n",
    "\n",
    "# Merge the two DataFrames on movieId and id\n",
    "df_cleaned['id'] = pd.to_numeric(df_cleaned['id'], errors='coerce')  # Ensure 'id' is numeric to match 'movieId'\n",
    "merged_df = pd.merge(df_cleaned, ratings_df, left_on='id', right_on='movieId', how='left')\n",
    "\n",
    "# Function to calculate the weighted vote average\n",
    "def update_vote_average(row):\n",
    "    if pd.isna(row['vote_average']) or row['vote_average'] == 0:\n",
    "        return row['average_rating']  # Replace with average_rating if vote_average is NaN or 0\n",
    "    if pd.notna(row['average_rating']):  # If both are present, calculate weighted average\n",
    "        vote_weight = row['vote_count'] if pd.notna(row['vote_count']) else 0\n",
    "        rating_weight = row['rating_count']\n",
    "        total_weight = vote_weight + rating_weight\n",
    "        return ((row['vote_average'] * vote_weight) + (row['average_rating'] * 2 * rating_weight)) / total_weight\n",
    "    return row['vote_average']  # If no update is needed, return the original vote_average\n",
    "\n",
    "# Apply the update_vote_average function to each row\n",
    "merged_df['vote_average_updated'] = merged_df.apply(update_vote_average, axis=1)\n",
    "merged_df['vote_count'] = merged_df['vote_count'] + merged_df['rating_count']\n",
    "\n",
    "# Drop extra columns like movieId from the merged DataFrame\n",
    "merged_df.drop(columns=['movieId', 'average_rating', 'rating_count'], inplace=True)\n",
    "\n",
    "merged_df.dropna(subset = [\"vote_average_updated\", \"vote_count\", \"revenue_budget_ratio\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Prepare Features and Labels\n",
    "X = merged_df[['release_month', 'release_day']]  # Use month, day, vote count as initial features\n",
    "\n",
    "# Prepare the multi-output labels (targets)\n",
    "y = merged_df[['revenue_budget_ratio', 'vote_average_updated', 'vote_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1575, 2) (1575, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Encode genres as categorical variables\n",
    "encoder = OneHotEncoder()\n",
    "genre_list = merged_df['parsed_genres'].apply(lambda x: ','.join(x) if isinstance(x, list) else '')\n",
    "genres_encoded = encoder.fit_transform(genre_list.values.reshape(-1, 1)).toarray()\n",
    "genre_feature_names = encoder.get_feature_names_out(['parsed_genres'])\n",
    "\n",
    "genres_encoded_df = pd.DataFrame(genres_encoded, columns=genre_feature_names)\n",
    "\n",
    "# Concatenate genre-encoded features to X\n",
    "X = pd.concat([X, genres_encoded_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in X before drop: 994728\n",
      "Missing values in y before drop: 0\n",
      "Missing values in X after drop: 0\n",
      "Missing values in y after drop: 0\n"
     ]
    }
   ],
   "source": [
    "# Step 7.1: Drop rows with NaN values\n",
    "# First, check for any NaN values in X or y\n",
    "print(\"Missing values in X before drop:\", X.isna().sum().sum())\n",
    "print(\"Missing values in y before drop:\", y.isna().sum().sum())\n",
    "\n",
    "# Drop rows with NaN values from both X and y\n",
    "X_cleaned = X.dropna()\n",
    "y_cleaned = y.dropna()\n",
    "\n",
    "# Ensure that X and y have the same number of rows after dropping NaNs\n",
    "X_cleaned, y_cleaned = X_cleaned.align(y_cleaned, join='inner', axis=0)\n",
    "\n",
    "print(\"Missing values in X after drop:\", X_cleaned.isna().sum().sum())\n",
    "print(\"Missing values in y after drop:\", y_cleaned.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(707, 1146) (707, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_cleaned.shape, y_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['release_month', 'release_day', 'parsed_genres_',\n",
       "       'parsed_genres_Action', 'parsed_genres_Action,Adventure',\n",
       "       'parsed_genres_Action,Adventure,Comedy',\n",
       "       'parsed_genres_Action,Adventure,Comedy,Crime,Thriller',\n",
       "       'parsed_genres_Action,Adventure,Comedy,Drama,Mystery',\n",
       "       'parsed_genres_Action,Adventure,Comedy,Drama,Western',\n",
       "       'parsed_genres_Action,Adventure,Comedy,Family,Fantasy',\n",
       "       ...\n",
       "       'parsed_genres_Thriller,Science Fiction,Mystery,Romance',\n",
       "       'parsed_genres_War',\n",
       "       'parsed_genres_War,Crime,Drama,Mystery,Romance,Thriller',\n",
       "       'parsed_genres_War,Drama', 'parsed_genres_War,Drama,History',\n",
       "       'parsed_genres_War,Drama,History,Adventure,Romance,Thriller',\n",
       "       'parsed_genres_War,History,Action,Adventure,Drama,Romance',\n",
       "       'parsed_genres_Western', 'parsed_genres_Western,Adventure',\n",
       "       'parsed_genres_Western,Thriller'],\n",
       "      dtype='object', length=1146)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        5789.0\n",
      "1        2536.0\n",
      "3        4231.0\n",
      "5        2513.0\n",
      "8       14025.0\n",
      "         ...   \n",
      "5351       30.0\n",
      "5362       93.0\n",
      "5369       58.0\n",
      "5370       86.0\n",
      "5372       95.0\n",
      "Name: vote_count, Length: 1575, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(y['vote_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Train-Test Split with cleaned data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cleaned, y_cleaned, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 9: Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for Revenue/Budget Ratio: 231.22914300458982\n",
      "Mean Squared Error for Vote Average: 0.7132189925012951\n",
      "Mean Squared Error for Vote Count: 107418073.22799663\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Train a RandomForest model for Multi-Output Regression\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 11: Evaluate the model\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate Mean Squared Error for each target\n",
    "mse_revenue = mean_squared_error(y_test['revenue_budget_ratio'], y_pred[:, 0])\n",
    "mse_vote_average = mean_squared_error(y_test['vote_average_updated'], y_pred[:, 1])\n",
    "mse_vote_count = mean_squared_error(y_test['vote_count'], y_pred[:, 2])\n",
    "\n",
    "print(f\"Mean Squared Error for Revenue/Budget Ratio: {mse_revenue}\")\n",
    "print(f\"Mean Squared Error for Vote Average: {mse_vote_average}\")\n",
    "print(f\"Mean Squared Error for Vote Count: {mse_vote_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       12.451801\n",
       "1        4.043035\n",
       "3        3.123947\n",
       "5        6.072311\n",
       "8        0.102218\n",
       "          ...    \n",
       "5351     3.237833\n",
       "5362     4.184134\n",
       "5369     5.833044\n",
       "5370     4.000000\n",
       "5372     1.097910\n",
       "Name: revenue_budget_ratio, Length: 1575, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y['revenue_budget_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        5789.0\n",
       "1        2536.0\n",
       "3        4231.0\n",
       "5        2513.0\n",
       "8       14025.0\n",
       "         ...   \n",
       "5351       30.0\n",
       "5362       93.0\n",
       "5369       58.0\n",
       "5370       86.0\n",
       "5372       95.0\n",
       "Name: vote_count, Length: 1575, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y['vote_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'catboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Fine-tuning model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcatboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CatBoostRegressor\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Step 10: Use CatBoost for Multi-Output Regression\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model_catboost \u001b[38;5;241m=\u001b[39m CatBoostRegressor(iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, loss_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultiRMSE\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'catboost'"
     ]
    }
   ],
   "source": [
    "# Fine-tuning model\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Step 10: Use CatBoost for Multi-Output Regression\n",
    "model_catboost = CatBoostRegressor(iterations=500, depth=10, learning_rate=0.1, loss_function='MultiRMSE', random_state=42, verbose=0)\n",
    "model_catboost.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 11: Evaluate CatBoost model\n",
    "y_pred_catboost = model_catboost.predict(X_test_scaled)\n",
    "\n",
    "# Calculate Mean Squared Error for each target\n",
    "mse_revenue_catboost = mean_squared_error(y_test['revenue_budget_ratio'], y_pred_catboost[:, 0])\n",
    "mse_vote_average_catboost = mean_squared_error(y_test['vote_average_updated'], y_pred_catboost[:, 1])\n",
    "mse_vote_count_catboost = mean_squared_error(y_test['vote_count'], y_pred_catboost[:, 2])\n",
    "\n",
    "print(f\"CatBoost - MSE for Revenue/Budget Ratio: {mse_revenue_catboost}\")\n",
    "print(f\"CatBoost - MSE for Vote Average: {mse_vote_average_catboost}\")\n",
    "print(f\"CatBoost - MSE for Vote Count: {mse_vote_count_catboost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "# Step 10: Use XGBoost for Multi-Output Regression\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "model_xgb = MultiOutputRegressor(xgb_model)\n",
    "model_xgb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 11: Evaluate XGBoost model\n",
    "y_pred_xgb = model_xgb.predict(X_test_scaled)\n",
    "\n",
    "# Calculate Mean Squared Error for each target\n",
    "mse_revenue_xgb = mean_squared_error(y_test['revenue_budget_ratio'], y_pred_xgb[:, 0])\n",
    "mse_vote_average_xgb = mean_squared_error(y_test['vote_average_updated'], y_pred_xgb[:, 1])\n",
    "mse_vote_count_xgb = mean_squared_error(y_test['vote_count'], y_pred_xgb[:, 2])\n",
    "\n",
    "print(f\"XGBoost - MSE for Revenue/Budget Ratio: {mse_revenue_xgb}\")\n",
    "print(f\"XGBoost - MSE for Vote Average: {mse_vote_average_xgb}\")\n",
    "print(f\"XGBoost - MSE for Vote Count: {mse_vote_count_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define parameter grid for CatBoost\n",
    "param_grid = {\n",
    "    'depth': [6, 8, 10],\n",
    "    'iterations': [100, 300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'l2_leaf_reg': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# Random search of parameters, using 3-fold cross-validation\n",
    "random_search_catboost = RandomizedSearchCV(CatBoostRegressor(loss_function='MultiRMSE', random_state=42), param_grid, n_iter=10, cv=3, verbose=0, random_state=42)\n",
    "\n",
    "# Fit the random search model\n",
    "random_search_catboost.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best Params (CatBoost):\", random_search_catboost.best_params_)\n",
    "print(\"Best Score (CatBoost):\", random_search_catboost.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Evaluate model with cross-validation (e.g., CatBoost)\n",
    "catboost_model = CatBoostRegressor(iterations=500, depth=10, learning_rate=0.1, loss_function='MultiRMSE', random_state=42, verbose=0)\n",
    "cv_scores = cross_val_score(catboost_model, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "print(\"Cross-Validation Scores (CatBoost):\", -cv_scores)\n",
    "print(\"Mean CV Score:\", -np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Define the Optuna objective function\n",
    "def objective(trial):\n",
    "    # Suggest values for hyperparameters\n",
    "    depth = trial.suggest_int('depth', 4, 10)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3)\n",
    "    iterations = trial.suggest_int('iterations', 100, 1000)\n",
    "    l2_leaf_reg = trial.suggest_float('l2_leaf_reg', 1, 10)\n",
    "\n",
    "    # Create the model with suggested hyperparameters\n",
    "    model = CatBoostRegressor(\n",
    "        depth=depth,\n",
    "        learning_rate=learning_rate,\n",
    "        iterations=iterations,\n",
    "        l2_leaf_reg=l2_leaf_reg,\n",
    "        loss_function='MultiRMSE',\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate the Mean Squared Error for the first target (revenue_budget_ratio)\n",
    "    mse = mean_squared_error(y_test['revenue_budget_ratio'], y_pred[:, 0])\n",
    "    \n",
    "    return mse\n",
    "\n",
    "# Step 2: Run the Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Step 3: Print the best parameters\n",
    "print(\"Best hyperparameters: \", study.best_params)\n",
    "\n",
    "# Step 4: Train the final model with the best parameters\n",
    "best_params = study.best_params\n",
    "model_catboost = CatBoostRegressor(\n",
    "    depth=best_params['depth'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    iterations=best_params['iterations'],\n",
    "    l2_leaf_reg=best_params['l2_leaf_reg'],\n",
    "    loss_function='MultiRMSE',\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "model_catboost.fit(X_train_scaled, y_train)\n",
    "y_pred_best = model_catboost.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the final model\n",
    "mse_final = mean_squared_error(y_test['revenue_budget_ratio'], y_pred_best[:, 0])\n",
    "print(f\"Final MSE: {mse_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameter search space for XGBoost\n",
    "    param = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5)\n",
    "    }\n",
    "\n",
    "    # Initialize and train the model\n",
    "    model_xgb = xgb.XGBRegressor(**param)\n",
    "    model_xgb.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predict and calculate MSE\n",
    "    y_pred = model_xgb.predict(X_test_scaled)\n",
    "    mse = mean_squared_error(y_test['revenue_budget_ratio'], y_pred[:, 0])\n",
    "\n",
    "    return mse\n",
    "\n",
    "# Run the study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best hyperparameters for XGBoost: \", study.best_params)\n",
    "\n",
    "# Train final model with best parameters\n",
    "best_params = study.best_params\n",
    "model_xgb_best = xgb.XGBRegressor(**best_params)\n",
    "model_xgb_best.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the final model\n",
    "y_pred_best = model_xgb_best.predict(X_test_scaled)\n",
    "mse_final_xgb = mean_squared_error(y_test['revenue_budget_ratio'], y_pred_best[:, 0])\n",
    "print(f\"Final MSE for XGBoost: {mse_final_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_multi(trial):\n",
    "    depth = trial.suggest_int('depth', 4, 10)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3)\n",
    "    iterations = trial.suggest_int('iterations', 100, 1000)\n",
    "    l2_leaf_reg = trial.suggest_float('l2_leaf_reg', 1, 10)\n",
    "\n",
    "    model = CatBoostRegressor(\n",
    "        depth=depth,\n",
    "        learning_rate=learning_rate,\n",
    "        iterations=iterations,\n",
    "        l2_leaf_reg=l2_leaf_reg,\n",
    "        loss_function='MultiRMSE',\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    mse_revenue = mean_squared_error(y_test['revenue_budget_ratio'], y_pred[:, 0])\n",
    "    mse_vote_average = mean_squared_error(y_test['vote_average_updated'], y_pred[:, 1])\n",
    "    mse_vote_count = mean_squared_error(y_test['vote_count'], y_pred[:, 2])\n",
    "\n",
    "    # Aggregate the MSE values for multi-objective optimization\n",
    "    return mse_revenue + mse_vote_average + mse_vote_count\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective_multi, n_trials=50)\n",
    "\n",
    "print(\"Best hyperparameters for multi-objective optimization: \", study.best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bir_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
