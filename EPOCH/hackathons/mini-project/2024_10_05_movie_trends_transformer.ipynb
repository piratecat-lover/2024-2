{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "gromwjigxP1e"
      },
      "outputs": [],
      "source": [
        "# Predicting Movie Success Metrics Using a Transformer Model\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Import transformer components\n",
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# For data preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "UKCFZ-1VyT17"
      },
      "outputs": [],
      "source": [
        "credits = pd.read_csv('/content/raw-data/credits.csv', engine='python', on_bad_lines='skip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "8lhl5c24yTr1"
      },
      "outputs": [],
      "source": [
        "keywords = pd.read_csv('/content/raw-data/keywords.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "UCWtWllDxP1j"
      },
      "outputs": [],
      "source": [
        "# Load datasets from the 'raw-data' folder\n",
        "movies_metadata = pd.read_csv('/content/raw-data/movies_metadata.csv', low_memory=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "GSl2Jtx5xP1k"
      },
      "outputs": [],
      "source": [
        "def parse_json_column(df, column_name):\n",
        "    \"\"\"Safely parse JSON-like strings in a DataFrame column.\"\"\"\n",
        "    def parse_json(x):\n",
        "        if pd.isna(x):\n",
        "            return []\n",
        "        try:\n",
        "            return ast.literal_eval(x)\n",
        "        except (ValueError, SyntaxError):\n",
        "            return []\n",
        "    df[column_name] = df[column_name].apply(parse_json)\n",
        "    return df\n",
        "\n",
        "# Parse columns\n",
        "credits = parse_json_column(credits, 'cast')\n",
        "credits = parse_json_column(credits, 'crew')\n",
        "keywords = parse_json_column(keywords, 'keywords')\n",
        "\n",
        "json_columns = ['genres', 'production_companies', 'production_countries', 'spoken_languages', 'belongs_to_collection']\n",
        "for column in json_columns:\n",
        "    movies_metadata = parse_json_column(movies_metadata, column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "4utMWThMxP1l"
      },
      "outputs": [],
      "source": [
        "# Extract main actors (top 3 billed actors)\n",
        "def get_top_actors(cast_list, n=3):\n",
        "    if isinstance(cast_list, list):\n",
        "        cast_list = sorted(cast_list, key=lambda x: x.get('order', 999))\n",
        "        actors = [member.get('name') for member in cast_list[:n] if member.get('name')]\n",
        "        return actors\n",
        "    return []\n",
        "\n",
        "credits['main_actors'] = credits['cast'].apply(get_top_actors)\n",
        "\n",
        "# Extract director\n",
        "def get_director(crew_list):\n",
        "    if isinstance(crew_list, list):\n",
        "        for member in crew_list:\n",
        "            if member.get('job') == 'Director':\n",
        "                return member.get('name')\n",
        "    return np.nan\n",
        "\n",
        "credits['director'] = credits['crew'].apply(get_director)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "RbyWlnHMxP1l"
      },
      "outputs": [],
      "source": [
        "def get_keywords(keywords_list):\n",
        "    return [kw.get('name') for kw in keywords_list if kw.get('name')]\n",
        "\n",
        "keywords['keyword_list'] = keywords['keywords'].apply(get_keywords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "UrJMWjnUxP1m"
      },
      "outputs": [],
      "source": [
        "# Drop weird values in movies_metadata['production_countries']\n",
        "\n",
        "movies_metadata = movies_metadata[movies_metadata['production_countries'].apply(lambda x: isinstance(x, list))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "WL4uLihuxP1n"
      },
      "outputs": [],
      "source": [
        "# Extract production countries\n",
        "def get_production_countries(production_countries_list):\n",
        "    return [country.get('iso_3166_1') for country in production_countries_list if country.get('iso_3166_1')]\n",
        "\n",
        "movies_metadata['production_countries_list'] = movies_metadata['production_countries'].apply(get_production_countries)\n",
        "\n",
        "# Extract original language\n",
        "movies_metadata['original_language'] = movies_metadata['original_language'].fillna('Unknown')\n",
        "\n",
        "# Determine if movie is part of a collection\n",
        "def has_collection(collection):\n",
        "    return 1 if isinstance(collection, dict) else 0\n",
        "\n",
        "movies_metadata['has_collection'] = movies_metadata['belongs_to_collection'].apply(has_collection)\n",
        "\n",
        "# Convert release_date to datetime\n",
        "movies_metadata['release_date'] = pd.to_datetime(movies_metadata['release_date'], errors='coerce')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "s6EhnmCOxP1o"
      },
      "outputs": [],
      "source": [
        "# Ensure 'id' columns are numeric\n",
        "movies_metadata['id'] = pd.to_numeric(movies_metadata['id'], errors='coerce')\n",
        "credits['id'] = pd.to_numeric(credits['id'], errors='coerce')\n",
        "keywords['id'] = pd.to_numeric(keywords['id'], errors='coerce')\n",
        "\n",
        "# Merge datasets\n",
        "df = movies_metadata.merge(credits[['id', 'main_actors', 'director']], on='id', how='left')\n",
        "df = df.merge(keywords[['id', 'keyword_list']], on='id', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "DUwd-MA8xP1w"
      },
      "outputs": [],
      "source": [
        "# Drop rows with missing release dates\n",
        "df = df.dropna(subset=['release_date'])\n",
        "\n",
        "# Fill missing lists with empty lists\n",
        "list_columns = ['main_actors', 'keyword_list', 'production_countries_list']\n",
        "for col in list_columns:\n",
        "    df[col] = df[col].apply(lambda x: x if isinstance(x, list) else [])\n",
        "\n",
        "# Fill missing director with 'Unknown'\n",
        "df['director'] = df['director'].fillna('Unknown')\n",
        "\n",
        "# Fill missing 'has_collection' with 0\n",
        "df['has_collection'] = df['has_collection'].fillna(0)\n",
        "\n",
        "# Convert budget and revenue to numeric\n",
        "df['budget'] = pd.to_numeric(df['budget'], errors='coerce').fillna(0)\n",
        "df['revenue'] = pd.to_numeric(df['revenue'], errors='coerce').fillna(0)\n",
        "df['popularity'] = pd.to_numeric(df['popularity'], errors='coerce').fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "6JBHS_VKxP1y"
      },
      "outputs": [],
      "source": [
        "# Avoid division by zero\n",
        "df['budget'] = df['budget'].replace(0, np.nan)\n",
        "df['rev_budget_ratio'] = df['revenue'] / df['budget']\n",
        "df['rev_budget_ratio'] = df['rev_budget_ratio'].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "df['budget'] = df['budget'].fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Pv2s7UNYxP1z"
      },
      "outputs": [],
      "source": [
        "# Target variables\n",
        "df['rating'] = pd.to_numeric(df['vote_average'], errors='coerce').fillna(0)\n",
        "df['popularity'] = df['popularity'].fillna(0)\n",
        "\n",
        "# Targets: revenue, rev_budget_ratio, rating, popularity\n",
        "targets = ['revenue', 'rev_budget_ratio', 'rating', 'popularity']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "MI487TEIxP1z"
      },
      "outputs": [],
      "source": [
        "def combine_text_features(row):\n",
        "    features = []\n",
        "\n",
        "    # Handle main_actors\n",
        "    actors = row['main_actors']\n",
        "    if isinstance(actors, list):\n",
        "        features.extend([str(actor) for actor in actors if actor])\n",
        "    elif pd.notnull(actors):\n",
        "        features.append(str(actors))\n",
        "\n",
        "    # Handle director\n",
        "    director = row['director']\n",
        "    if pd.notnull(director):\n",
        "        features.append(str(director))\n",
        "    else:\n",
        "        features.append('Unknown')\n",
        "\n",
        "    # Handle keyword_list\n",
        "    keywords = row['keyword_list']\n",
        "    if isinstance(keywords, list):\n",
        "        features.extend([str(keyword) for keyword in keywords if keyword])\n",
        "    elif pd.notnull(keywords):\n",
        "        features.append(str(keywords))\n",
        "\n",
        "    # Handle production_countries_list\n",
        "    countries = row['production_countries_list']\n",
        "    if isinstance(countries, list):\n",
        "        features.extend([str(country) for country in countries if country])\n",
        "    elif pd.notnull(countries):\n",
        "        features.append(str(countries))\n",
        "\n",
        "    # Handle original_language\n",
        "    language = row['original_language']\n",
        "    if pd.notnull(language):\n",
        "        features.append(str(language))\n",
        "    else:\n",
        "        features.append('Unknown')\n",
        "\n",
        "    # Handle has_collection\n",
        "    has_collection = row['has_collection']\n",
        "    if has_collection == 1:\n",
        "        features.append('Collection')\n",
        "    else:\n",
        "        features.append('No Collection')\n",
        "\n",
        "    # Combine features into a single string\n",
        "    return ' '.join(features)\n",
        "\n",
        "df['text_input'] = df.apply(combine_text_features, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2xnCjDrxP10",
        "outputId": "21d0c469-db1f-4b7a-8f41-b216f8d980dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Using BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "_m9vcI4kxP12"
      },
      "outputs": [],
      "source": [
        "# Tokenize the text inputs\n",
        "def tokenize_text(text):\n",
        "    return tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=128,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "df['tokenized'] = df['text_input'].apply(tokenize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "Z-qYNQc8xP12"
      },
      "outputs": [],
      "source": [
        "class MovieDataset(Dataset):\n",
        "    def __init__(self, df, targets):\n",
        "        self.inputs = df['tokenized'].tolist()\n",
        "        self.targets = df[targets].values.astype(float)\n",
        "        self.targets = torch.tensor(self.targets, dtype=torch.float)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_ids = self.inputs[idx]['input_ids'].squeeze()\n",
        "        attention_mask = self.inputs[idx]['attention_mask'].squeeze()\n",
        "        targets = self.targets[idx]\n",
        "        return input_ids, attention_mask, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "19W8FsFNxP13"
      },
      "outputs": [],
      "source": [
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=False)\n",
        "\n",
        "# Reset indices\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "-taMjnDPxP13"
      },
      "outputs": [],
      "source": [
        "batch_size = 8  # Reduce this number\n",
        "\n",
        "train_dataset = MovieDataset(train_df, targets)\n",
        "test_dataset = MovieDataset(test_df, targets)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "5LK8MX0gxP13"
      },
      "outputs": [],
      "source": [
        "class MovieTransformerModel(nn.Module):\n",
        "    def __init__(self, num_targets):\n",
        "        super(MovieTransformerModel, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, num_targets)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        # Use [CLS] token representation\n",
        "        cls_output = outputs[1]\n",
        "        cls_output = self.dropout(cls_output)\n",
        "        output = self.fc(cls_output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bo6PFWh5xP14",
        "outputId": "87371f5e-3d2b-4890-aca7-7af35dad74b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MovieTransformerModel(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc): Linear(in_features=768, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "num_targets = len(targets)\n",
        "\n",
        "model = MovieTransformerModel(num_targets)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "wTgLSf7TxP19"
      },
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smvq6GTDxP2A",
        "outputId": "b37d1193-c0c4-4e5c-a8a7-6b3364e95e40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1'20, iteration 1, Loss: 8007386702807040.0\n",
            "Epoch 1'20, iteration 2, Loss: 4803686844858368.0\n",
            "Epoch 1'20, iteration 3, Loss: 2461660237266944.0\n",
            "Epoch 1'20, iteration 4, Loss: 2004398960017408.0\n",
            "Epoch 1'20, iteration 5, Loss: 2066703366225920.0\n",
            "Epoch 1'20, iteration 6, Loss: 7588902806224896.0\n",
            "Epoch 1'20, iteration 7, Loss: 18428708519936.0\n",
            "Epoch 1'20, iteration 8, Loss: 362507118051328.0\n",
            "Epoch 1'20, iteration 9, Loss: 50094787264512.0\n",
            "Epoch 1'20, iteration 10, Loss: 12492790038528.0\n",
            "Epoch 1'20, iteration 11, Loss: 3324081602560.0\n",
            "Epoch 1'20, iteration 12, Loss: 724788817428480.0\n",
            "Epoch 1'20, iteration 13, Loss: 1088382025334784.0\n",
            "Epoch 1'20, iteration 14, Loss: 1472699457077248.0\n",
            "Epoch 1'20, iteration 15, Loss: 1231719104512.0\n",
            "Epoch 1'20, iteration 16, Loss: 160317858381824.0\n",
            "Epoch 1'20, iteration 17, Loss: 3034320470016.0\n",
            "Epoch 1'20, iteration 18, Loss: 1697419393761280.0\n",
            "Epoch 1'20, iteration 19, Loss: 7513904624173056.0\n",
            "Epoch 1'20, iteration 20, Loss: 4092709232967680.0\n",
            "Epoch 1'20, iteration 21, Loss: 4727358296686592.0\n",
            "Epoch 1'20, iteration 22, Loss: 424700391981056.0\n",
            "Epoch 1'20, iteration 23, Loss: 677827309469696.0\n",
            "Epoch 1'20, iteration 24, Loss: 18496358449152.0\n",
            "Epoch 1'20, iteration 25, Loss: 487185354588160.0\n",
            "Epoch 1'20, iteration 26, Loss: 2671200148914176.0\n",
            "Epoch 1'20, iteration 27, Loss: 40300798017536.0\n",
            "Epoch 1'20, iteration 28, Loss: 1447038369660928.0\n",
            "Epoch 1'20, iteration 29, Loss: 1914993444388864.0\n",
            "Epoch 1'20, iteration 30, Loss: 44197998493696.0\n",
            "Epoch 1'20, iteration 31, Loss: 33641923084288.0\n",
            "Epoch 1'20, iteration 32, Loss: 1606280456175616.0\n",
            "Epoch 1'20, iteration 33, Loss: 1.960988651814912e+16\n",
            "Epoch 1'20, iteration 34, Loss: 393850849853440.0\n",
            "Epoch 1'20, iteration 35, Loss: 442279256915968.0\n",
            "Epoch 1'20, iteration 36, Loss: 79162538721280.0\n",
            "Epoch 1'20, iteration 37, Loss: 2622971961147392.0\n",
            "Epoch 1'20, iteration 38, Loss: 19551819399168.0\n",
            "Epoch 1'20, iteration 39, Loss: 909965392871424.0\n",
            "Epoch 1'20, iteration 40, Loss: 2358707455262720.0\n",
            "Epoch 1'20, iteration 41, Loss: 455016250867712.0\n",
            "Epoch 1'20, iteration 42, Loss: 1090301875716096.0\n",
            "Epoch 1'20, iteration 43, Loss: 370441298378752.0\n",
            "Epoch 1'20, iteration 44, Loss: 2.0174990397669376e+16\n",
            "Epoch 1'20, iteration 45, Loss: 2.1609520211951616e+16\n",
            "Epoch 1'20, iteration 46, Loss: 3968109010485248.0\n",
            "Epoch 1'20, iteration 47, Loss: 8646931492372480.0\n",
            "Epoch 1'20, iteration 48, Loss: 124803512008704.0\n",
            "Epoch 1'20, iteration 49, Loss: 322494430969856.0\n",
            "Epoch 1'20, iteration 50, Loss: 9.719776153564453\n",
            "Epoch 1'20, iteration 51, Loss: 152117708849152.0\n",
            "Epoch 1'20, iteration 52, Loss: 465141804040192.0\n",
            "Epoch 1'20, iteration 53, Loss: 46547093946368.0\n",
            "Epoch 1'20, iteration 54, Loss: 2088180081754112.0\n",
            "Epoch 1'20, iteration 55, Loss: 916613331156992.0\n",
            "Epoch 1'20, iteration 56, Loss: 2813781344256.0\n",
            "Epoch 1'20, iteration 57, Loss: 7287447104782336.0\n",
            "Epoch 1'20, iteration 58, Loss: 747428194025472.0\n",
            "Epoch 1'20, iteration 59, Loss: 1142819393634304.0\n",
            "Epoch 1'20, iteration 60, Loss: 2.6460537596411904e+16\n",
            "Epoch 1'20, iteration 61, Loss: 679594420076544.0\n",
            "Epoch 1'20, iteration 62, Loss: 6574638226210816.0\n",
            "Epoch 1'20, iteration 63, Loss: 1920692731772928.0\n",
            "Epoch 1'20, iteration 64, Loss: 507640102780928.0\n",
            "Epoch 1'20, iteration 65, Loss: 419144650457088.0\n",
            "Epoch 1'20, iteration 66, Loss: 3230931024347136.0\n",
            "Epoch 1'20, iteration 67, Loss: 2119571863502848.0\n",
            "Epoch 1'20, iteration 68, Loss: 104033587036160.0\n",
            "Epoch 1'20, iteration 69, Loss: 374040782962688.0\n",
            "Epoch 1'20, iteration 70, Loss: 5919904628736.0\n",
            "Epoch 1'20, iteration 71, Loss: 15.334343910217285\n",
            "Epoch 1'20, iteration 72, Loss: 141860420976640.0\n",
            "Epoch 1'20, iteration 73, Loss: 3.708352931771187e+16\n",
            "Epoch 1'20, iteration 74, Loss: 2.005196964941005e+16\n",
            "Epoch 1'20, iteration 75, Loss: 303336058257408.0\n"
          ]
        }
      ],
      "source": [
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    iteration = 1\n",
        "    for input_ids, attention_mask, target in train_loader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"Epoch {epoch+1}'{epochs}, iteration {iteration}, Loss: {loss.item()}\")\n",
        "        iteration += 1\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Training Loss: {avg_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0OpfAQkxP2D"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "total_loss = 0\n",
        "with torch.no_grad():\n",
        "    for input_ids, attention_mask, target in test_loader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = criterion(outputs, target)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    print(f'Test Loss: {avg_loss:.4f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "bir_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}