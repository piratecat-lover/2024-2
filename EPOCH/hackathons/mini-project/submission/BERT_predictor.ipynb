{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "gromwjigxP1e"
      },
      "outputs": [],
      "source": [
        "# Predicting Movie Success Metrics Using a Transformer Model\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Import transformer components\n",
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# For data preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "UKCFZ-1VyT17"
      },
      "outputs": [],
      "source": [
        "credits = pd.read_csv('/content/raw-data/credits.csv', engine='python', on_bad_lines='skip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "8lhl5c24yTr1"
      },
      "outputs": [],
      "source": [
        "keywords = pd.read_csv('/content/raw-data/keywords.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "UCWtWllDxP1j"
      },
      "outputs": [],
      "source": [
        "# Load datasets from the 'raw-data' folder\n",
        "movies_metadata = pd.read_csv('/content/raw-data/movies_metadata.csv', low_memory=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ratings = pd.read_csv('/content/ratings_average.csv')"
      ],
      "metadata": {
        "id": "soFd4dcLrNqx"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "GSl2Jtx5xP1k"
      },
      "outputs": [],
      "source": [
        "def parse_json_column(df, column_name):\n",
        "    \"\"\"Safely parse JSON-like strings in a DataFrame column.\"\"\"\n",
        "    def parse_json(x):\n",
        "        if pd.isna(x):\n",
        "            return []\n",
        "        try:\n",
        "            return ast.literal_eval(x)\n",
        "        except (ValueError, SyntaxError):\n",
        "            return []\n",
        "    df[column_name] = df[column_name].apply(parse_json)\n",
        "    return df\n",
        "\n",
        "# Parse columns\n",
        "credits = parse_json_column(credits, 'cast')\n",
        "credits = parse_json_column(credits, 'crew')\n",
        "keywords = parse_json_column(keywords, 'keywords')\n",
        "\n",
        "json_columns = ['genres', 'production_companies', 'production_countries', 'spoken_languages', 'belongs_to_collection']\n",
        "for column in json_columns:\n",
        "    movies_metadata = parse_json_column(movies_metadata, column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "4utMWThMxP1l"
      },
      "outputs": [],
      "source": [
        "# Extract main actors (top 3 billed actors)\n",
        "def get_top_actors(cast_list, n=3):\n",
        "    if isinstance(cast_list, list):\n",
        "        cast_list = sorted(cast_list, key=lambda x: x.get('order', 999))\n",
        "        actors = [member.get('name') for member in cast_list[:n] if member.get('name')]\n",
        "        return actors\n",
        "    return []\n",
        "\n",
        "credits['main_actors'] = credits['cast'].apply(get_top_actors)\n",
        "\n",
        "# Extract director\n",
        "def get_director(crew_list):\n",
        "    if isinstance(crew_list, list):\n",
        "        for member in crew_list:\n",
        "            if member.get('job') == 'Director':\n",
        "                return member.get('name')\n",
        "    return np.nan\n",
        "\n",
        "credits['director'] = credits['crew'].apply(get_director)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "RbyWlnHMxP1l"
      },
      "outputs": [],
      "source": [
        "def get_keywords(keywords_list):\n",
        "    return [kw.get('name') for kw in keywords_list if kw.get('name')]\n",
        "\n",
        "keywords['keyword_list'] = keywords['keywords'].apply(get_keywords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "UrJMWjnUxP1m"
      },
      "outputs": [],
      "source": [
        "# Drop weird values in movies_metadata['production_countries']\n",
        "\n",
        "movies_metadata = movies_metadata[movies_metadata['production_countries'].apply(lambda x: isinstance(x, list))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WL4uLihuxP1n",
        "outputId": "32a4b15b-99e8-48c9-c9fa-87c4e4adcac8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-200-90674df00a19>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  movies_metadata['production_countries_list'] = movies_metadata['production_countries'].apply(get_production_countries)\n",
            "<ipython-input-200-90674df00a19>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  movies_metadata['original_language'] = movies_metadata['original_language'].fillna('Unknown')\n",
            "<ipython-input-200-90674df00a19>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  movies_metadata['has_collection'] = movies_metadata['belongs_to_collection'].apply(has_collection)\n",
            "<ipython-input-200-90674df00a19>:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  movies_metadata['release_date'] = pd.to_datetime(movies_metadata['release_date'], errors='coerce')\n"
          ]
        }
      ],
      "source": [
        "# Extract production countries\n",
        "def get_production_countries(production_countries_list):\n",
        "    return [country.get('iso_3166_1') for country in production_countries_list if country.get('iso_3166_1')]\n",
        "\n",
        "movies_metadata['production_countries_list'] = movies_metadata['production_countries'].apply(get_production_countries)\n",
        "\n",
        "# Extract original language\n",
        "movies_metadata['original_language'] = movies_metadata['original_language'].fillna('Unknown')\n",
        "\n",
        "# Determine if movie is part of a collection\n",
        "def has_collection(collection):\n",
        "    return 1 if isinstance(collection, dict) else 0\n",
        "\n",
        "movies_metadata['has_collection'] = movies_metadata['belongs_to_collection'].apply(has_collection)\n",
        "\n",
        "# Convert release_date to datetime\n",
        "movies_metadata['release_date'] = pd.to_datetime(movies_metadata['release_date'], errors='coerce')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6EhnmCOxP1o",
        "outputId": "bd31990b-5591-49e8-ecab-59afdd6e11e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-201-79442f1e03bf>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  movies_metadata['id'] = pd.to_numeric(movies_metadata['id'], errors='coerce')\n"
          ]
        }
      ],
      "source": [
        "# Ensure 'id' columns are numeric\n",
        "movies_metadata['id'] = pd.to_numeric(movies_metadata['id'], errors='coerce')\n",
        "credits['id'] = pd.to_numeric(credits['id'], errors='coerce')\n",
        "keywords['id'] = pd.to_numeric(keywords['id'], errors='coerce')\n",
        "\n",
        "# Merge datasets\n",
        "df = movies_metadata.merge(credits[['id', 'main_actors', 'director']], on='id', how='left')\n",
        "df = df.merge(keywords[['id', 'keyword_list']], on='id', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUwd-MA8xP1w",
        "outputId": "cbb0d144-bfb0-4b86-e360-a11d56852c61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-202-46b2db88db07>:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['release_year'] = df['release_date'].dt.year.astype(str)\n",
            "<ipython-input-202-46b2db88db07>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['release_month'] = df['release_date'].dt.month.astype(str)\n",
            "<ipython-input-202-46b2db88db07>:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['release_day'] = df['release_date'].dt.day.astype(str)\n",
            "<ipython-input-202-46b2db88db07>:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[col] = df[col].apply(lambda x: x if isinstance(x, list) else [])\n",
            "<ipython-input-202-46b2db88db07>:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['director'] = df['director'].fillna('Unknown')\n",
            "<ipython-input-202-46b2db88db07>:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['has_collection'] = df['has_collection'].fillna(0)\n",
            "<ipython-input-202-46b2db88db07>:25: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['budget'] = pd.to_numeric(df['budget'], errors='coerce').fillna(0)\n",
            "<ipython-input-202-46b2db88db07>:26: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['revenue'] = pd.to_numeric(df['revenue'], errors='coerce').fillna(0)\n",
            "<ipython-input-202-46b2db88db07>:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['popularity'] = pd.to_numeric(df['popularity'], errors='coerce').fillna(0)\n"
          ]
        }
      ],
      "source": [
        "# Assuming you have already loaded your DataFrame 'df'\n",
        "\n",
        "# Ensure 'release_date' is in datetime format\n",
        "df['release_date'] = pd.to_datetime(df['release_date'], errors='coerce')\n",
        "\n",
        "# Drop rows with missing 'release_date' if necessary\n",
        "df = df.dropna(subset=['release_date'])\n",
        "\n",
        "# Extract 'release_year', 'release_month', 'release_day'\n",
        "df['release_year'] = df['release_date'].dt.year.astype(str)\n",
        "df['release_month'] = df['release_date'].dt.month.astype(str)\n",
        "df['release_day'] = df['release_date'].dt.day.astype(str)\n",
        "# Fill missing lists with empty lists\n",
        "list_columns = ['main_actors', 'keyword_list', 'production_countries_list']\n",
        "for col in list_columns:\n",
        "    df[col] = df[col].apply(lambda x: x if isinstance(x, list) else [])\n",
        "\n",
        "# Fill missing director with 'Unknown'\n",
        "df['director'] = df['director'].fillna('Unknown')\n",
        "\n",
        "# Fill missing 'has_collection' with 0\n",
        "df['has_collection'] = df['has_collection'].fillna(0)\n",
        "\n",
        "# Convert budget and revenue to numeric\n",
        "df['budget'] = pd.to_numeric(df['budget'], errors='coerce').fillna(0)\n",
        "df['revenue'] = pd.to_numeric(df['revenue'], errors='coerce').fillna(0)\n",
        "df['popularity'] = pd.to_numeric(df['popularity'], errors='coerce').fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "6JBHS_VKxP1y"
      },
      "outputs": [],
      "source": [
        "# 2. Cleaning Budget and Revenue\n",
        "def clean_numeric(value):\n",
        "    try:\n",
        "        val = int(value)\n",
        "        if val == 0:\n",
        "            return np.nan  # Treat 0 as missing value\n",
        "        return val\n",
        "    except (ValueError, TypeError):\n",
        "        return np.nan\n",
        "\n",
        "df['budget_cleaned'] = df['budget'].apply(clean_numeric)\n",
        "df['revenue_cleaned'] = df['revenue'].apply(clean_numeric)\n",
        "\n",
        "# Step 5: Target variable (revenue/budget ratio)\n",
        "df['rev_budget_ratio'] = df['revenue_cleaned'] / df['budget_cleaned']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the first quartile (Q1) and third quartile (Q3)\n",
        "Q1 = df['rev_budget_ratio'].quantile(0.25)\n",
        "Q3 = df['rev_budget_ratio'].quantile(0.75)\n",
        "\n",
        "# Calculate the interquartile range (IQR)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define the lower and upper bounds for outliers\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Filter the dataframe to remove outliers\n",
        "df = df[(df['rev_budget_ratio'] >= lower_bound) &\n",
        "                                    (df['rev_budget_ratio'] <= upper_bound)]"
      ],
      "metadata": {
        "id": "DG7DDJaLq-YZ"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the two DataFrames on movieId and id\n",
        "df['id'] = pd.to_numeric(df['id'], errors='coerce')  # Ensure 'id' is numeric to match 'movieId'\n",
        "df = pd.merge(df, ratings, left_on='id', right_on='movieId', how='left')\n",
        "\n",
        "# Function to calculate the weighted vote average\n",
        "def update_vote_average(row):\n",
        "    if pd.isna(row['vote_average']) or row['vote_average'] == 0:\n",
        "        return row['average_rating']  # Replace with average_rating if vote_average is NaN or 0\n",
        "    if pd.notna(row['average_rating']):  # If both are present, calculate weighted average\n",
        "        vote_weight = row['vote_count'] if pd.notna(row['vote_count']) else 0\n",
        "        rating_weight = row['rating_count']\n",
        "        total_weight = vote_weight + rating_weight\n",
        "        return ((row['vote_average'] * vote_weight) + (row['average_rating'] * 2 * rating_weight)) / total_weight\n",
        "    return row['vote_average']  # If no update is needed, return the original vote_average\n",
        "\n",
        "# Apply the update_vote_average function to each row\n",
        "df['vote_average_updated'] = df.apply(update_vote_average, axis=1)\n",
        "df['vote_count'] = df['vote_count'] + df['rating_count']\n",
        "\n",
        "# Drop extra columns like movieId from the merged DataFrame\n",
        "df.drop(columns=['movieId', 'average_rating', 'rating_count'], inplace=True)\n",
        "\n",
        "df.dropna(subset = [\"vote_average_updated\", \"vote_count\", \"rev_budget_ratio\"], inplace=True)"
      ],
      "metadata": {
        "id": "CflVyToyrDVi"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "Pv2s7UNYxP1z"
      },
      "outputs": [],
      "source": [
        "# Ensure 'vote_average' and 'popularity' are numeric and handle missing values\n",
        "df['rating'] = pd.to_numeric(df['vote_average'], errors='coerce').fillna(0)\n",
        "df['popularity'] = pd.to_numeric(df['popularity'], errors='coerce').fillna(0)\n",
        "\n",
        "# Define target variables\n",
        "targets = ['rev_budget_ratio', 'rating', 'popularity']\n",
        "\n",
        "# Ensure targets are float and handle infinities and NaNs\n",
        "df[targets] = df[targets].astype(float)\n",
        "df[targets] = np.nan_to_num(df[targets], nan=0.0, posinf=0.0, neginf=0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "id": "MI487TEIxP1z"
      },
      "outputs": [],
      "source": [
        "def combine_text_features(row):\n",
        "    features = []\n",
        "\n",
        "    # Handle main_actors\n",
        "    actors = row['main_actors']\n",
        "    if isinstance(actors, list):\n",
        "        features.extend([str(actor) for actor in actors if actor])\n",
        "    elif pd.notnull(actors):\n",
        "        features.append(str(actors))\n",
        "\n",
        "    # Handle director\n",
        "    director = row['director']\n",
        "    if pd.notnull(director):\n",
        "        features.append(str(director))\n",
        "    else:\n",
        "        features.append('Unknown')\n",
        "\n",
        "    # Handle keyword_list\n",
        "    keywords = row['keyword_list']\n",
        "    if isinstance(keywords, list):\n",
        "        features.extend([str(keyword) for keyword in keywords if keyword])\n",
        "    elif pd.notnull(keywords):\n",
        "        features.append(str(keywords))\n",
        "\n",
        "    # Handle production_countries_list\n",
        "    countries = row['production_countries_list']\n",
        "    if isinstance(countries, list):\n",
        "        features.extend([str(country) for country in countries if country])\n",
        "    elif pd.notnull(countries):\n",
        "        features.append(str(countries))\n",
        "\n",
        "    # Handle original_language\n",
        "    language = row['original_language']\n",
        "    if pd.notnull(language):\n",
        "        features.append(str(language))\n",
        "    else:\n",
        "        features.append('Unknown')\n",
        "\n",
        "    # Handle has_collection\n",
        "    has_collection = row['has_collection']\n",
        "    if has_collection == 1:\n",
        "        features.append('Collection')\n",
        "    else:\n",
        "        features.append('No Collection')\n",
        "\n",
        "    # Include release date information\n",
        "    release_year = row['release_year']\n",
        "    release_month = row['release_month']\n",
        "    release_day = row['release_day']\n",
        "    features.append('ReleaseYear_' + release_year)\n",
        "    features.append('ReleaseMonth_' + release_month)\n",
        "    features.append('ReleaseDay_' + release_day)\n",
        "\n",
        "    # Combine features into a single string\n",
        "    return ' '.join(features)\n",
        "\n",
        "df['text_input'] = df.apply(combine_text_features, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2xnCjDrxP10",
        "outputId": "f36c8359-50f7-4300-dfb8-47fe05a805fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Using BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "_m9vcI4kxP12"
      },
      "outputs": [],
      "source": [
        "# Tokenize the text inputs\n",
        "def tokenize_text(text):\n",
        "    return tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=128,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "df['tokenized'] = df['text_input'].apply(tokenize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "id": "Z-qYNQc8xP12"
      },
      "outputs": [],
      "source": [
        "class MovieDataset(Dataset):\n",
        "    def __init__(self, df, targets):\n",
        "        self.texts = df['text_input'].tolist()\n",
        "        self.targets = df[targets].values.astype(float)\n",
        "        self.targets = torch.tensor(self.targets, dtype=torch.float)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.max_length = 128\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        # Tokenize on-the-fly\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        input_ids = encoding['input_ids'].squeeze(0)  # Remove batch dimension\n",
        "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
        "        targets = self.targets[idx]\n",
        "        return input_ids, attention_mask, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "19W8FsFNxP13"
      },
      "outputs": [],
      "source": [
        "# Split Data\n",
        "# First, split the data into training+validation and test sets\n",
        "train_full_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=False)\n",
        "\n",
        "# Then, split the training data into training and validation sets\n",
        "train_df, val_df = train_test_split(train_full_df, test_size=0.1, random_state=42, shuffle=False)\n",
        "\n",
        "# # Truncate train_df to 500 rows for brevity\n",
        "# train_df = train_df.head(500)\n",
        "\n",
        "# Reset indices\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "zNbxcRlL5ZaY"
      },
      "outputs": [],
      "source": [
        "# Normalize targets with MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler on the training targets\n",
        "train_df[targets] = scaler.fit_transform(train_df[targets])\n",
        "\n",
        "# Transform validation and test targets\n",
        "val_df[targets] = scaler.transform(val_df[targets])\n",
        "test_df[targets] = scaler.transform(test_df[targets])\n",
        "\n",
        "# scaler2 = StandardScaler()\n",
        "# train_df[targets] = scaler2.fit_transform(train_df[targets])\n",
        "\n",
        "# scaler3 = MinMaxScaler()\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-taMjnDPxP13",
        "outputId": "b950b7e0-27a3-43d3-a58a-0097b24b5a30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Create datasets\n",
        "train_dataset = MovieDataset(train_df, targets)\n",
        "val_dataset = MovieDataset(val_df, targets)\n",
        "test_dataset = MovieDataset(test_df, targets)\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 8\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "il6HJDdSCXL1",
        "outputId": "4f847680-e781-4ebd-af92-d72d83b72a72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loader Size: 124\n",
            "Validation Loader Size: 14\n",
            "Test Loader Size: 35\n"
          ]
        }
      ],
      "source": [
        "# Print train_loader, val_loader, test-loader sizes\n",
        "print(\"Train Loader Size:\", len(train_loader))\n",
        "print(\"Validation Loader Size:\", len(val_loader))\n",
        "print(\"Test Loader Size:\", len(test_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "id": "5LK8MX0gxP13"
      },
      "outputs": [],
      "source": [
        "# Model Definition\n",
        "class MovieTransformerModel(nn.Module):\n",
        "    def __init__(self, num_targets):\n",
        "        super(MovieTransformerModel, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, num_targets)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        # Use [CLS] token representation\n",
        "        cls_output = outputs[1]\n",
        "        cls_output = self.dropout(cls_output)\n",
        "        output = self.fc(cls_output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bo6PFWh5xP14",
        "outputId": "dece29ba-4646-4241-e665-a7ba885a89ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MovieTransformerModel(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc): Linear(in_features=768, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 217
        }
      ],
      "source": [
        "# Initialize model\n",
        "num_targets = len(targets)\n",
        "model = MovieTransformerModel(num_targets)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "id": "wTgLSf7TxP19"
      },
      "outputs": [],
      "source": [
        "# Loss Function and Optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "id": "yENpbyND_bHH"
      },
      "outputs": [],
      "source": [
        "# Early Stopping Implementation\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0.001):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How many epochs to wait after last time validation loss improved.\n",
        "            min_delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "        self.best_state_dict = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            self.best_state_dict = model.state_dict()\n",
        "        elif val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.best_state_dict = model.state_dict()\n",
        "            self.counter = 0  # Reset counter when improvement occurs\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smvq6GTDxP2A",
        "outputId": "e93cfb4c-48ab-4e3c-c29d-cefdf8e6f15e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Iteration 0, Loss: 0.11718712002038956\n",
            "Epoch 1/1, Iteration 1, Loss: 0.07155942916870117\n",
            "Epoch 1/1, Iteration 2, Loss: 0.053454700857400894\n",
            "Epoch 1/1, Iteration 3, Loss: 0.023946762084960938\n",
            "Epoch 1/1, Iteration 4, Loss: 0.0246958639472723\n",
            "Epoch 1/1, Iteration 5, Loss: 0.07407664507627487\n",
            "Epoch 1/1, Iteration 6, Loss: 0.014067312702536583\n",
            "Epoch 1/1, Iteration 7, Loss: 0.02000158280134201\n",
            "Epoch 1/1, Iteration 8, Loss: 0.013367947190999985\n",
            "Epoch 1/1, Iteration 9, Loss: 0.01789010316133499\n",
            "Epoch 1/1, Iteration 10, Loss: 0.03334253281354904\n",
            "Epoch 1/1, Iteration 11, Loss: 0.031421490013599396\n",
            "Epoch 1/1, Iteration 12, Loss: 0.047140683978796005\n",
            "Epoch 1/1, Iteration 13, Loss: 0.039095327258110046\n",
            "Epoch 1/1, Iteration 14, Loss: 0.03222070261836052\n",
            "Epoch 1/1, Iteration 15, Loss: 0.06271322071552277\n",
            "Epoch 1/1, Iteration 16, Loss: 0.05057935416698456\n",
            "Epoch 1/1, Iteration 17, Loss: 0.016565149649977684\n",
            "Epoch 1/1, Iteration 18, Loss: 0.03681910037994385\n",
            "Epoch 1/1, Iteration 19, Loss: 0.03920947015285492\n",
            "Epoch 1/1, Iteration 20, Loss: 0.0746508240699768\n",
            "Epoch 1/1, Iteration 21, Loss: 0.03760182857513428\n",
            "Epoch 1/1, Iteration 22, Loss: 0.049433570355176926\n",
            "Epoch 1/1, Iteration 23, Loss: 0.018965665251016617\n",
            "Epoch 1/1, Iteration 24, Loss: 0.03159024938941002\n",
            "Epoch 1, Training Loss: 0.0413, Validation Loss: 0.0179\n"
          ]
        }
      ],
      "source": [
        "# Training Loop with Early Stopping\n",
        "# Initialize early stopping\n",
        "early_stopping = EarlyStopping(patience=5, min_delta=0.0001)\n",
        "\n",
        "epochs = 1  # Increase the number of epochs to allow the model to see more data over time\n",
        "batch_size = 4  # Adjust based on your computational resources\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Sample 500 random indices from the training dataset\n",
        "    num_samples = 100\n",
        "    indices = np.random.choice(len(train_dataset), num_samples, replace=False)\n",
        "    sampler = SubsetRandomSampler(indices)\n",
        "    # Create a DataLoader with the sampler\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
        "\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    iteration = 0\n",
        "    for input_ids, attention_mask, target in train_loader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Iteration {iteration}, Loss: {loss.item()}\")\n",
        "        iteration += 1\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    # Validation step remains the same\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attention_mask, target in val_loader:\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, target)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "    # Early stopping\n",
        "    early_stopping(avg_val_loss, model)\n",
        "\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered. Stopping training.\")\n",
        "        # Load the best model weights\n",
        "        model.load_state_dict(early_stopping.best_state_dict)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "id": "z0OpfAQkxP2D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8c0a4a4-e708-40e7-ce67-3a7d456084e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0192\n",
            "MSE for revenue: 55844221344546816.0000\n",
            "MSE for rev_budget_ratio: 3.8730\n",
            "MSE for rating: 0.8336\n",
            "MSE for popularity: 48.8635\n"
          ]
        }
      ],
      "source": [
        "# Evaluation on test set\n",
        "model.eval()\n",
        "total_test_loss = 0\n",
        "\n",
        "num_samples = 100\n",
        "indices = np.random.choice(len(test_dataset), num_samples, replace=False)\n",
        "sampler = SubsetRandomSampler(indices)\n",
        "# Create a DataLoader with the sampler\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=sampler, shuffle = False)\n",
        "with torch.no_grad():\n",
        "    for input_ids, attention_mask, target in test_loader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = criterion(outputs, target)\n",
        "        total_test_loss += loss.item()\n",
        "\n",
        "    avg_test_loss = total_test_loss / len(test_loader)\n",
        "    print(f'Test Loss: {avg_test_loss:.4f}')\n",
        "\n",
        "# Inverse Transforming Predictions (Optional)\n",
        "# Collect all predictions and targets\n",
        "all_outputs = []\n",
        "all_targets = []\n",
        "with torch.no_grad():\n",
        "    for input_ids, attention_mask, target in test_loader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        all_outputs.append(outputs.cpu().numpy())\n",
        "        all_targets.append(target.cpu().numpy())\n",
        "\n",
        "# Concatenate the outputs and targets\n",
        "all_outputs = np.concatenate(all_outputs, axis=0)\n",
        "all_targets = np.concatenate(all_targets, axis=0)\n",
        "\n",
        "# Inverse transform to original scale\n",
        "outputs_original = scaler.inverse_transform(all_outputs)\n",
        "targets_original = scaler.inverse_transform(all_targets)\n",
        "\n",
        "# Compute metrics in original scale\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "for i, target_name in enumerate(targets):\n",
        "    mse = mean_squared_error(targets_original[:, i], outputs_original[:, i])\n",
        "    print(f'MSE for {target_name}: {mse:.4f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "bir_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}