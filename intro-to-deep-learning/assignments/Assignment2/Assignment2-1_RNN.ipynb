{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\johng\\.conda\\envs\\deep-learning-24-windows\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\johng\\.conda\\envs\\deep-learning-24-windows\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-17.0.0-cp310-cp310-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\johng\\.conda\\envs\\deep-learning-24-windows\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\johng\\.conda\\envs\\deep-learning-24-windows\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\johng\\.conda\\envs\\deep-learning-24-windows\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.6.1,>=2023.1.0 (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.10.10-cp310-cp310-win_amd64.whl.metadata (7.8 kB)\n",
      "Collecting huggingface-hub>=0.22.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\johng\\.conda\\envs\\deep-learning-24-windows\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\johng\\.conda\\envs\\deep-learning-24-windows\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.9.11-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp310-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.1-cp310-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\johng\\.conda\\envs\\deep-learning-24-windows\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp310-cp310-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.15.5-cp310-cp310-win_amd64.whl.metadata (63 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\johng\\.conda\\envs\\deep-learning-24-windows\\lib\\site-packages (from huggingface-hub>=0.22.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\johng\\.conda\\envs\\deep-learning-24-windows\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\johng\\.conda\\envs\\deep-learning-24-windows\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\johng\\.conda\\envs\\deep-learning-24-windows\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\johng\\.conda\\envs\\deep-learning-24-windows\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\johng\\.conda\\envs\\deep-learning-24-windows\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\johng\\.conda\\envs\\deep-learning-24-windows\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\johng\\.conda\\envs\\deep-learning-24-windows\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\johng\\.conda\\envs\\deep-learning-24-windows\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\johng\\.conda\\envs\\deep-learning-24-windows\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp->datasets)\n",
      "  Downloading propcache-0.2.0-cp310-cp310-win_amd64.whl.metadata (7.9 kB)\n",
      "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
      "Downloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "   ---------------------------------------- 0.0/9.9 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 2.4/9.9 MB 12.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.7/9.9 MB 11.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.3/9.9 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.7/9.9 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.9/9.9 MB 11.4 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Downloading aiohttp-3.10.10-cp310-cp310-win_amd64.whl (381 kB)\n",
      "Downloading huggingface_hub-0.26.1-py3-none-any.whl (447 kB)\n",
      "Downloading pyarrow-17.0.0-cp310-cp310-win_amd64.whl (25.1 MB)\n",
      "   ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 2.4/25.1 MB 11.2 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 4.7/25.1 MB 11.4 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 7.1/25.1 MB 11.5 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 9.4/25.1 MB 11.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 12.1/25.1 MB 11.6 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 14.4/25.1 MB 11.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 16.8/25.1 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 19.1/25.1 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.8/25.1 MB 11.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.1/25.1 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.1/25.1 MB 11.5 MB/s eta 0:00:00\n",
      "Downloading regex-2024.9.11-cp310-cp310-win_amd64.whl (274 kB)\n",
      "Downloading safetensors-0.4.5-cp310-none-win_amd64.whl (285 kB)\n",
      "Downloading tokenizers-0.20.1-cp310-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 11.4 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-win_amd64.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-win_amd64.whl (50 kB)\n",
      "Downloading multidict-6.1.0-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Downloading yarl-1.15.5-cp310-cp310-win_amd64.whl (85 kB)\n",
      "Downloading propcache-0.2.0-cp310-cp310-win_amd64.whl (44 kB)\n",
      "Installing collected packages: xxhash, safetensors, regex, pyarrow, propcache, multidict, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, transformers, datasets\n",
      "Successfully installed aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 async-timeout-4.0.3 datasets-3.0.1 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.6.1 huggingface-hub-0.26.1 multidict-6.1.0 multiprocess-0.70.16 propcache-0.2.0 pyarrow-17.0.0 regex-2024.9.11 safetensors-0.4.5 tokenizers-0.20.1 transformers-4.45.2 xxhash-3.5.0 yarl-1.15.5\n"
     ]
    }
   ],
   "source": [
    "# install dependencies\n",
    "!pip install datasets transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.004300 002 Deep Learning <br> Assignment #2 Part 1: Training Recurrent Neural Network (RNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) Data Science & AI Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by DongHyeok Lee, October 2024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "The goal of this assignment is to progressively train deeper and more accurate models using PyTorch.\n",
    "\n",
    "This notebook uses the [imdb](https://huggingface.co/datasets/stanfordnlp/imdb) dataset to be used with python experiments. The IMDB dataset is a popular benchmark for sentiment analysis tasks in natural language processing. It contains 50,000 movie reviews from the Internet Movie Database (IMDB), split evenly into 25,000 reviews for training and 25,000 for testing. Each review is labeled as either positive (1) or negative (0), making it a binary classification problem. The dataset is well-balanced, with an equal number of positive and negative reviews in both the training and testing sets.\n",
    "\n",
    "**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n",
    "\n",
    "### Submitting your work:\n",
    "\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.  \n",
    "Once you have done problems, run the _CollectSubmission.sh_ script with your **Student number** as input argument. <br>\n",
    "This will produce a compressed file called _[Your student number].tar.gz_. Please submit this file on ETL. &nbsp;&nbsp; (Usage: ./_CollectSubmission.sh_ &nbsp; 20\\*\\*-\\*\\*\\*\\*\\*)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook, we will focus on two main sections:\n",
    "\n",
    "2-1-1. Implement the forward and backward propagation for a GRU (Gated Recurrent Unit)\n",
    "\n",
    "- We will dive deep into the internal mechanics of a GRU cell\n",
    "- You will implement the forward pass, understanding how the update gate, reset gate, and new memory content interact\n",
    "- You'll also implement the backward pass, deriving and calculating the gradients for each component\n",
    "- This exercise will give you a thorough understanding of how GRUs process sequential data\n",
    "\n",
    "2-1-2. Training Multi-Layer RNN with PyTorch Module\n",
    "\n",
    "- We'll use PyTorch to build a complete GRU-based model for sentiment analysis\n",
    "- You'll learn how to:\n",
    "  - Create a custom PyTorch Module for the GRU model\n",
    "  - Set up the training loop, including data loading and batching\n",
    "  - Implement the forward pass of the entire model\n",
    "  - Train the model on the IMDB dataset\n",
    "  - Evaluate the model's performance on a test set\n",
    "\n",
    "By completing these two sections, you'll gain both a low-level understanding of GRU operations and practical experience in using GRUs for a real-world NLP task. This combination of theoretical knowledge and practical application will deepen your understanding of recurrent neural networks and their use in sequence modeling tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2-1-1 | Implement the forward and backward propagation for a GRU (Gated Recurrent Unit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows the architecture and explanation of GRU.  \n",
    "Referring to this, calculate the forward and backward passes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/GRU.webp\" alt=\"Image description\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key equations of the Gated Recurrent Unit (GRU):\n",
    "\n",
    "1. Update Gate:\n",
    "\n",
    "$z_t = \\sigma( (U_z \\cdot h_{t-1} + b_{zh}) + (W_z \\cdot x_{t} + b_{zw}))$\n",
    "\n",
    "2. Reset Gate:\n",
    "\n",
    "$r_t = \\sigma( (U_r \\cdot h_{t-1} + b_{rh}) + (W_r \\cdot x_{t} + b_{rw}))$\n",
    "\n",
    "3. Current Memory Content:\n",
    "\n",
    "$\\tilde{h}_t = \\tanh((W \\cdot x_{t} + b_{w}) + r_t * (U \\cdot h_{t-1} + b_{h}))$\n",
    "\n",
    "4. Final Memory:\n",
    "\n",
    "$h_t = z_t * h_{t-1} + (1-z_t) * \\tilde{h}_t$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\sigma$ is the sigmoid function\n",
    "- $\\tanh$ is the hyperbolic tangent function\n",
    "- $*$ denotes element-wise multiplication\n",
    "- $\\cdot$ represents matrix multiplication\n",
    "- $W_z, W_r, W, U_z, U_r, U$ are weight matrices\n",
    "- $b_{zh}, b_{rh}, b_{h}, b_{zw}, b_{rw}, b_{w}$ are bias vectors\n",
    "- $x_t$ is the input at the current time step\n",
    "- $h_{t-1}$ is the hidden state from the previous time step\n",
    "- $h_t$ is the hidden state at the current time step\n",
    "\n",
    "Tip. The derivative of the tanh function:\n",
    "\n",
    "$\\frac{d}{dx} \\tanh(x) = 1 - \\tanh^2(x)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def gru_forward(\n",
    "    input: torch.Tensor,  # (batch_size, input_size), dtype=torch.double\n",
    "    hidden: torch.Tensor,  # (batch_size, hidden_size), dtype=torch.double\n",
    "    weight_ih: torch.Tensor,  # (3 * hidden_size, input_size), dtype=torch.double\n",
    "    weight_hh: torch.Tensor,  # (3 * hidden_size, hidden_size), dtype=torch.double\n",
    "    bias_ih: torch.Tensor,  # (3 * hidden_size), dtype=torch.double\n",
    "    bias_hh: torch.Tensor,  # (3 * hidden_size), dtype=torch.double\n",
    "):\n",
    "    ##############################################################################\n",
    "    #                          IMPLEMENT YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    # Update Gate\n",
    "    z_t = torch.sigmoid(torch.mm(weight_ih[0:hidden.size(1), :].t(), input.t()) + torch.mm(weight_hh[0:hidden.size(1), :].t(), hidden.t()) + bias_ih[0:hidden.size(1)].reshape(-1, 1) + bias_hh[0:hidden.size(1)].reshape(-1, 1))\n",
    "    # Reset Gate\n",
    "    r_t = torch.sigmoid(torch.mm(weight_ih[hidden.size(1):2*hidden.size(1), :].t(), input.t()) + torch.mm(weight_hh[hidden.size(1):2*hidden.size(1), :].t(), hidden.t()) + bias_ih[hidden.size(1):2*hidden.size(1)].reshape(-1, 1) + bias_hh[hidden.size(1):2*hidden.size(1)].reshape(-1, 1))\n",
    "    # Current Memory Content\n",
    "    h_bar = torch.tanh(torch.mm(weight_ih[2*hidden.size(1):3*hidden.size(1), :].t(), input.t()) + r_t * (torch.mm(weight_hh[2*hidden.size(1):3*hidden.size(1), :].t(), hidden.t())) + bias_ih[2*hidden.size(1):3*hidden.size(1)].reshape(-1, 1) + bias_hh[2*hidden.size(1):3*hidden.size(1)].reshape(-1, 1))\n",
    "    # Final Memory\n",
    "    h_t = (1 - z_t) * h_bar + z_t * hidden.t()\n",
    "    ##############################################################################\n",
    "    #                          END OF YOUR CODE                                  #\n",
    "    ##############################################################################\n",
    "    # output: torch.Tensor  # (batch_size, hidden_size)\n",
    "    output = h_t\n",
    "    return output\n",
    "\n",
    "\n",
    "def gru_backward(\n",
    "    grad_output: torch.Tensor,  # (batch_size, hidden_size), dtype=torch.double\n",
    "    #\n",
    "    input: torch.Tensor,  # (batch_size, input_size), dtype=torch.double\n",
    "    hidden: torch.Tensor,  # (batch_size, hidden_size), dtype=torch.double\n",
    "    weight_ih: torch.Tensor,  # (3 * hidden_size, input_size), dtype=torch.double\n",
    "    weight_hh: torch.Tensor,  # (3 * hidden_size, hidden_size), dtype=torch.double\n",
    "    bias_ih: torch.Tensor,  # (3 * hidden_size), dtype=torch.double\n",
    "    bias_hh: torch.Tensor,  # (3 * hidden_size), dtype=torch.double\n",
    "    # IMPORTANT!\n",
    "    # Thhe order of weight_ih, weight_hh, bias_ih, bias_hh (3 hidden_size, input_size)\n",
    "    # is reset, update, new (current)\"\n",
    "):\n",
    "    ##############################################################################\n",
    "    #                          IMPLEMENT YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    # Calculate backward pass through GRU\n",
    "    # Backprop through the final memory\n",
    "    grad_h_t = grad_output.t()\n",
    "    \n",
    "    ##############################################################################\n",
    "    #                          END OF YOUR CODE                                  #\n",
    "    ##############################################################################\n",
    "    grad_hidden: torch.Tensor  # (batch_size, hidden_size)\n",
    "    grad_weight_ih: torch.Tensor  # (3 * hidden_size, input_size)\n",
    "    grad_weight_hh: torch.Tensor  # (3 * hidden_size, hidden_size)\n",
    "    grad_bias_ih: torch.Tensor  # (3 * hidden_size)\n",
    "    grad_bias_hh: torch.Tensor  # (3 * hidden_size)\n",
    "    return grad_hidden, grad_weight_ih, grad_weight_hh, grad_bias_ih, grad_bias_hh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Important!</font>  \n",
    "Write the final result to `model_checkpoints/gru.py` and submit it.  \n",
    "Errors resulting from modifications to any part of the script other than the function implementation sections will be considered as a failure to submit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2-1-2 | Training Multi-Layer RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, GRU is already implemented in PyTorch [link](https://pytorch.org/docs/stable/generated/torch.nn.GRUCell.html). In this problem, we will use the pre-implemented GRU cell to create a model and train it on the IMDB dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you will implement and train a Gated Recurrent Unit (GRU) model for sentiment analysis using the IMDB dataset. Your task is to:\n",
    "\n",
    "1. Implement the GRUModel class:\n",
    "\n",
    "   - The class should inherit from nn.Module\n",
    "   - Initialize the model with appropriate layers (embedding, GRU, and output layers)\n",
    "   - Implement the forward pass\n",
    "\n",
    "2. Set up the training process:\n",
    "\n",
    "   - Choose appropriate hyperparameters (e.g., embedding dimension, hidden dimension, number of layers)\n",
    "   - Initialize the model, loss function, and optimizer\n",
    "   - Implement the training loop using the provided train() function\n",
    "\n",
    "3. Evaluate the model:\n",
    "   - Use the provided evaluate() function to test your model on the test dataset\n",
    "   - Report the final training loss, test loss, and test accuracy\n",
    "\n",
    "Your goal is to achieve the highest possible accuracy on the test set. Experiment with different hyperparameters and model architectures to improve your results.\n",
    "\n",
    "Note: The data loading and preprocessing steps have been provided for you. Focus on implementing the model and training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "def load_data_and_tokenizer(max_length: int = 256):\n",
    "    dataset = load_dataset(\"imdb\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "\n",
    "    tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "    tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "\n",
    "    encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "    encoded_dataset.set_format(type=\"torch\")\n",
    "\n",
    "    return encoded_dataset, tokenizer\n",
    "\n",
    "\n",
    "def get_dataloader(encoded_dataset, batch_size):\n",
    "    train_dataloader = DataLoader(\n",
    "        encoded_dataset[\"train\"], shuffle=True, batch_size=batch_size, drop_last=True\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        encoded_dataset[\"test\"], batch_size=batch_size, drop_last=True\n",
    "    )\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    optimizer: Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    ") -> float:\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    total_loss = 0\n",
    "    tqdm_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    for batch in tqdm_bar:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        #\n",
    "        tqdm_bar.set_postfix(loss=loss.item())\n",
    "        tqdm_bar.update(1)\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    ") -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    tqdm_bar = tqdm(dataloader, desc=\"Evaluating\")\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm_bar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].float().to(device)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.round(torch.sigmoid(outputs))\n",
    "            correct += (predictions == labels).float().sum()\n",
    "            #\n",
    "            tqdm_bar.set_postfix(loss=loss.item())\n",
    "            tqdm_bar.update(1)\n",
    "    accuracy = correct / (len(dataloader.dataset))\n",
    "    return total_loss / len(dataloader), accuracy.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. setup dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load encoded dataset and tokenizer  \n",
    "you can check the detail of code in `src/assign1/load_data.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "\n",
    "encoded_dataset, tokenizer = load_data_and_tokenizer(max_length=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. EDA of dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"tokenizer vocab size : {tokenizer.vocab_size}, \\n token of [PAD] : {tokenizer.pad_token_id}, \\n token of [UNK] : {tokenizer.unk_token_id} \\n token of [CLS] : {tokenizer.cls_token_id} \\n token of [SEP] : {tokenizer.sep_token_id} \\n token of [MASK] : {tokenizer.mask_token_id}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data\n",
    "print(encoded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data detail\n",
    "print(encoded_dataset[\"train\"][224])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. setup dataloader and check validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataloader, test_dataloader = get_dataloader(\n",
    "    encoded_dataset, batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(f\"num of train data batches : {len(train_dataloader)}\")\n",
    "print(f\"num of test data batches : {len(test_dataloader)}\")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    assert isinstance(batch, dict)\n",
    "    assert \"input_ids\" in batch\n",
    "    assert \"attention_mask\" in batch\n",
    "    assert \"label\" in batch\n",
    "    assert batch[\"input_ids\"].shape[0] == batch[\"attention_mask\"].shape[0] == batch_size\n",
    "    assert batch[\"input_ids\"].shape[-1] == max_length\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    assert isinstance(batch, dict)\n",
    "    assert \"input_ids\" in batch\n",
    "    assert \"attention_mask\" in batch\n",
    "    assert \"label\" in batch\n",
    "    assert batch[\"input_ids\"].shape[0] == batch[\"attention_mask\"].shape[0] == batch_size\n",
    "    assert batch[\"input_ids\"].shape[-1] == max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. Define Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        embed_dim,\n",
    "        hidden_dim,\n",
    "        num_layers,\n",
    "        is_bidirectional,\n",
    "        output_dim=1,\n",
    "        dropout=0.2,\n",
    "    ):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.is_bidirectional = is_bidirectional\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            tokenizer.vocab_size, embed_dim, padding_idx=tokenizer.pad_token_id\n",
    "        )\n",
    "        self.gru = nn.GRU(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=is_bidirectional,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim * (2 if is_bidirectional else 1), output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        embedded = self.dropout(self.embedding(input_ids))\n",
    "        lengths = attention_mask.sum(dim=1)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        _, hidden = self.gru(packed_embedded)\n",
    "\n",
    "        if self.is_bidirectional:\n",
    "            hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1, :, :]\n",
    "\n",
    "        output = self.fc(hidden)\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with changing the parameters and submit best model\n",
    "\n",
    "**Train at least 5 different models with varying parameter combinations and report the results.  \n",
    "This instruction is asking you to:**\n",
    "\n",
    "1. Modify the given parameters to create different model configurations.\n",
    "2. Train at least 5 distinct models, each with a unique combination of these parameters.\n",
    "3. Run experiments with these different models.\n",
    "4. Collect and analyze the results from each experiment.\n",
    "5. Prepare a short report that compares and contrasts the performance of these different model configurations at `model_checkpoints/assignment2-1-2/report.md`  \n",
    "   !!Tip. Write it briefly. Length and content are not part of the grading score.!!\n",
    "6. **Submit all trained models and configs, including the best-performing one**  \n",
    "   <font color='red'>The scores will be assigned in order based on the highest score, and a perfect score will be given for accuracy of 88% or above</font>\n",
    "\n",
    "The goal is to understand how different parameter settings affect the model's performance on the given task (likely sentiment analysis on the IMDB dataset). This process is a crucial part of machine learning research and development, often referred to as hyperparameter tuning or model optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpu or gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# or if you have MPS\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class ExperimentConfig(BaseModel):\n",
    "    config_name: str  # feel free to name your model as you like, it will be used for saving model\n",
    "\n",
    "    # about env\n",
    "    seed: int  # for simplicity, we use common seed for all experiments\n",
    "\n",
    "    # about dataset\n",
    "    batch_size: int\n",
    "\n",
    "    # about model\n",
    "    embed_dim: int\n",
    "    hidden_dim: int\n",
    "    num_layers: int\n",
    "    output_dim: int\n",
    "    is_bidirectional: bool\n",
    "    dropout: float\n",
    "\n",
    "    # about optimizer\n",
    "    optimizer: Optional[str] = Field(None)\n",
    "    optimizer_params: dict = Field(default_factory=dict)\n",
    "    epochs: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "OptimizerClass = torch.optim.Adam  # < ----- set this parameter\n",
    "optimizer_params = {\"lr\": 1e-3}  # < ----- set this parameter\n",
    "\n",
    "model_config = ExperimentConfig(\n",
    "    config_name=\"gru\",  # <---- will be used for saving model\n",
    "    #\n",
    "    seed=1,  # < ----- set this parameter\n",
    "    #\n",
    "    batch_size=32,  # < ----- set this parameter\n",
    "    #\n",
    "    embed_dim=128,  # < ----- set this parameter\n",
    "    hidden_dim=128,  # < ----- set this parameter\n",
    "    num_layers=2,  # < ----- set this parameter\n",
    "    is_bidirectional=False,  # < ----- set this parameter\n",
    "    dropout=0.2,  # < ----- set this parameter\n",
    "    optimizer_params=optimizer_params,\n",
    "    #\n",
    "    epochs=10,  # < ----- set this parameter\n",
    "    #\n",
    "    output_dim=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(model_config.seed)\n",
    "torch.cuda.manual_seed(model_config.seed)\n",
    "torch.cuda.manual_seed_all(model_config.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "tokenizer = tokenizer\n",
    "\n",
    "model = GRUModel(\n",
    "    tokenizer=tokenizer,\n",
    "    embed_dim=model_config.embed_dim,\n",
    "    hidden_dim=model_config.hidden_dim,\n",
    "    num_layers=model_config.num_layers,\n",
    "    is_bidirectional=model_config.is_bidirectional,\n",
    "    dropout=model_config.dropout,\n",
    ")\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = OptimizerClass(model.parameters(), **model_config.optimizer_params)\n",
    "\n",
    "# set optimizer name\n",
    "model_config.optimizer = optimizer.__class__.__name__\n",
    "\n",
    "train_dataloader, test_dataloader = get_dataloader(\n",
    "    encoded_dataset, batch_size=model_config.batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(model_config.epochs):\n",
    "    train_final_loss = train(model, train_dataloader, optimizer, criterion, device)\n",
    "test_loss, test_accuracy = evaluate(model, test_dataloader, criterion, device)\n",
    "\n",
    "print(\n",
    "    f\"train_final_loss : {train_final_loss}, test_loss : {test_loss}, test_accuracy : {test_accuracy}\"\n",
    ")\n",
    "# ---- save model and config\n",
    "from pathlib import Path\n",
    "\n",
    "save_path = Path(\"./model_checkpoints/assignment2-1-2\")\n",
    "if not save_path.exists():\n",
    "    save_path.mkdir(parents=True)\n",
    "\n",
    "save_dict = {\n",
    "    \"test_loss\": test_loss,\n",
    "    \"test_accuracy\": test_accuracy,\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "}\n",
    "\n",
    "save_dict.update(model_config.dict())\n",
    "\n",
    "model_path = (\n",
    "    save_path\n",
    "    / f\"{model_config.config_name}_test_loss_{test_loss:.4f}_test_accuracy_{test_accuracy:.4f}.pth\"\n",
    ")\n",
    "torch.save(save_dict, model_path)\n",
    "print(f\"model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please provide your analysis on each hyper-parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "deep-learning-24-windows",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
