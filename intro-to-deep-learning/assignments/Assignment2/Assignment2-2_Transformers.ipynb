{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.004300 002 Deep Learning Assignment #2<br> Part 2: Vision Transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) Data Science & AI Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by DongHyeok Lee, October 2024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For understanding of this work, please carefully look at given PDF file.**\n",
    "\n",
    "In this notebook, you will learn,\n",
    "\n",
    "1. Implementing SwinTransformer:\n",
    "\n",
    "- SwinTransformer is a variant of the Vision Transformer that uses hierarchical feature representation and shifted window-based self-attention mechanisms.\n",
    "- You will implement key components of the SwinTransformer, including window partitioning, shifted window attention mechanisms, and hierarchical feature extraction processes.\n",
    "- This exercise will provide you with a deep understanding of the inner workings of state-of-the-art vision models.\n",
    "\n",
    "2. Comparing ViT and Swin-ViT with CIFAR-10:\n",
    "\n",
    "- You will compare the performance of ViT and Swin-ViT models using the CIFAR-10 dataset.\n",
    "- You'll utilize the Hugging Face library to easily implement and train both models.\n",
    "- You'll compare the models in various aspects such as model architecture, training speed, and accuracy, to understand the strengths and weaknesses of each model.\n",
    "- This comparative analysis will help you develop the ability to choose appropriate models for various vision tasks.\n",
    "\n",
    "**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n",
    "\n",
    "### Submitting your work:\n",
    "\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the assignment focuses on utilizing the Hugging Face library to train and compare Vision Transformer (ViT) and Swin Transformer (Swin-ViT) models. The primary objectives are:\n",
    "\n",
    "1. Performance and Efficiency Comparison:\n",
    "\n",
    "   - Compare the performance of ViT and Swin-ViT on the same task (e.g., image classification on CIFAR-10).\n",
    "   - Analyze the differences in terms of:\n",
    "     a) Accuracy: Evaluate which model achieves higher classification accuracy.\n",
    "     b) Parameters: Compare the number of trainable parameters in each model.\n",
    "     c) FLOPs (Floating Point Operations): Assess the computational efficiency of each model.\n",
    "   - Explain the reasons behind the observed differences, considering the architectural distinctions between ViT and Swin-ViT.\n",
    "\n",
    "2. Hyperparameter Tuning for Optimal Performance:\n",
    "   - Experiment with various hyperparameters for both ViT and Swin-ViT models to achieve the best possible performance.\n",
    "   - Parameters to tune may include:\n",
    "     a) Learning rate and learning rate schedule\n",
    "     b) Batch size\n",
    "     c) Number of attention heads\n",
    "     d) Embedding dimensions\n",
    "     e) Number of layers\n",
    "     f) Dropout rates\n",
    "   - Document the impact of different hyperparameter configurations on model performance.\n",
    "   - Identify the optimal set of hyperparameters for each model that yields the highest accuracy on the given task.\n",
    "\n",
    "By completing this section, you will gain hands-on experience in:\n",
    "\n",
    "- Implementing and training state-of-the-art vision models using the Hugging Face library.\n",
    "- Conducting comparative analysis of different transformer architectures for computer vision tasks.\n",
    "- Practicing hyperparameter tuning to optimize model performance.\n",
    "- Critically analyzing the trade-offs between model complexity, computational efficiency, and accuracy.\n",
    "\n",
    "This exercise will deepen your understanding of modern vision transformers and equip you with practical skills in model selection and optimization for real-world computer vision applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2-1 Implementing SwinTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/swin_transformer_architecture.png\" alt=\"Image description\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completing a generally functional Swin Transformer module can be challenging.  \n",
    "<font color=red>**Therefore, in this problem, we aim to simplify the conditions. The conditions are as follows:**<font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/\bwindow-attention.png\" alt=\"Image description\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedure:\n",
    "\n",
    "1. Apply Self-Window Attention:\n",
    "\n",
    "- Pass the input tensor X of shape $(B \\times H \\times W \\times C)$ through a self-window attention mechanism with $n$ heads and a window size of $w$\n",
    "\n",
    "2. Concatenate Attention Outputs and Apply MLP:\n",
    "\n",
    "- Concatenate the outputs from the $n$ attention heads along the channel dimension.\n",
    "- Pass the concatenated result through an MLP to adjust the channel size back to the original dimension $C$\n",
    "\n",
    "3. Residual Connection:\n",
    "\n",
    "- Add the original input $X$ to the output from the previous step to form a residual connection.\n",
    "\n",
    "#### Details:\n",
    "\n",
    "a. Self-Attention within Square Windows:\n",
    "\n",
    "- The self-attention mechanism operates within square windows of size $ w \\times w $\n",
    "\n",
    "b. Window Shifting:\n",
    "\n",
    "- Apply window shifts that are equal in size to the window dimensions, effectively sliding the windows across the input tensor.\n",
    "\n",
    "c. Zero Padding:\n",
    "\n",
    "- If necessary, add zero-padding to the **right** and **bottom** edges of the input tensor to ensure it can be evenly divided into windows.\n",
    "\n",
    "d. Calculation of $q$, $k$, and $v$:\n",
    "\n",
    "- Compute the query ($q$), key ($k$), and value ($v$) tensors using the provided **IdentityMLP**.\n",
    "- Do not include additional biases, normalization layers, or dropout during this computation.\n",
    "\n",
    "e. Channel-Wise Concatenation and Channel Size Adjustment:\n",
    "\n",
    "- Concatenate the outputs from the $n$ self-attention operations along the channel axis.\n",
    "- Pass the concatenated tensor through the **HeaderConcatMLP** to adjust the channel size to match that of the input $X$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "# use identity mlp when calculating q, k, v\n",
    "class IdentityMLP(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Linear(dim, dim)\n",
    "        self.initialize_weights_ones()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.mlp(x)\n",
    "\n",
    "    def initialize_weights_ones(self):\n",
    "        nn.init.ones_(self.mlp.weight)\n",
    "        nn.init.ones_(self.mlp.bias)\n",
    "\n",
    "\n",
    "# use HeaderConcatMLP when merging heads\n",
    "class HeaderConcatMLP(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Linear(in_dim, out_dim)\n",
    "        self.initialize_weights_ones()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.mlp(x)\n",
    "\n",
    "    def initialize_weights_ones(self):\n",
    "        nn.init.ones_(self.mlp.weight)\n",
    "        nn.init.ones_(self.mlp.bias)\n",
    "\n",
    "\n",
    "def cal_window_transformer_block(\n",
    "    x,\n",
    "    #\n",
    "    window_size: Tuple[int, int],\n",
    "    num_heads: int,\n",
    "):\n",
    "    B, H, W, C = x.shape\n",
    "    assert num_heads > 0, \"num_heads must be greater than 0\"\n",
    "    assert (\n",
    "        window_size[0] > 0 and window_size[1] < H and window_size[1] < W\n",
    "    ), \"window_size must be less than image size\"\n",
    "\n",
    "    ##############################################################################\n",
    "    #                          IMPLEMENT YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "\n",
    "    ##############################################################################\n",
    "    #                          END OF YOUR CODE                                  #\n",
    "    ##############################################################################\n",
    "\n",
    "    output: torch.Tensor = x\n",
    "    B_, H_, W_, C_ = output.shape\n",
    "    assert (B_, H_, W_, C_) == (\n",
    "        B,\n",
    "        H,\n",
    "        W,\n",
    "        C,\n",
    "    ), \"output shape should be same as input shape\"\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Important!</font>  \n",
    "Write the final result to `model_checkpoints/cal_window_transformer_block.py` and submit it.  \n",
    "Errors resulting from modifications to any part of the script other than the function implementation sections will be considered as a failure to submit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2-2 Vision Transformer for image classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, it aims to train ViT-family models for image classification using the CIFAR-10 dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load cifar 10 dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "\n",
    "def load_cifar10(data_dir, image_size: tuple[int, int] = (224, 224)):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.5, 0.5, 0.5],\n",
    "                std=[0.5, 0.5, 0.5],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=True, download=True, transform=transform\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=False, download=True, transform=transform\n",
    "    )\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "def get_dataloader(train_dataset, test_dataset, batch_size, num_workers):\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers\n",
    "    )\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "\n",
    "def visualize_tensor_image(\n",
    "    tensor_image: torch.Tensor,\n",
    "    mean: list[float] = [0.5, 0.5, 0.5],\n",
    "    std: list[float] = [0.5, 0.5, 0.5],\n",
    "    size: tuple[int, int] = (32, 32),\n",
    ") -> Image:\n",
    "    assert len(tensor_image.shape) == 3, \"Tensor must have 3 dimensions\"\n",
    "    tensor_image = tensor_image.permute(1, 2, 0)\n",
    "    tensor_image = tensor_image.cpu().numpy()\n",
    "    tensor_image = (tensor_image * std) + mean\n",
    "    tensor_image = (tensor_image * 255).clip(0, 255).astype(np.uint8)\n",
    "    tensor_image = cv2.resize(tensor_image, size)\n",
    "    return Image.fromarray(tensor_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (32, 32)\n",
    "train_dataset, test_dataset = load_cifar10(\"./data\", image_size)\n",
    "class_names = train_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from random import randint\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(16):\n",
    "    rv_index = randint(0, len(train_dataset))\n",
    "    rv_image, rv_index = train_dataset[rv_index]\n",
    "    rv_label = class_names[rv_index]\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(\n",
    "        visualize_tensor_image(\n",
    "            rv_image, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], size=image_size\n",
    "        )\n",
    "    )\n",
    "    plt.title(f\"{rv_label} ({rv_index})\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load dataloader and check validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_classes = 10\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, drop_last=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, drop_last=True\n",
    ")\n",
    "\n",
    "# check data validation\n",
    "for images, labels in train_loader:\n",
    "    assert images.shape == (\n",
    "        batch_size,\n",
    "        3,\n",
    "        image_size[0],\n",
    "        image_size[1],\n",
    "    ), \"images shape is not correct\"\n",
    "    assert labels.shape == (batch_size,), \"labels shape is not correct\"\n",
    "    assert labels.max() < num_classes, \"labels are out of range\"\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    assert images.shape == (\n",
    "        batch_size,\n",
    "        3,\n",
    "        image_size[0],\n",
    "        image_size[1],\n",
    "    ), \"images shape is not correct\"\n",
    "    assert labels.shape == (batch_size,), \"labels shape is not correct\"\n",
    "    assert labels.max() < num_classes, \"labels are out of range\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train ViT and Swin ViT with HF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face provides pre-implemented versions of ViT and SwinViT, making it convenient to create and use these models. Below are some simple examples, and you can find more detailed information through the following links:\n",
    "\n",
    "1. Vision Transformer (ViT):\n",
    "\n",
    "   - Hugging Face documentation: [ViT Model](https://huggingface.co/docs/transformers/model_doc/vit)\n",
    "   - Example usage:\n",
    "\n",
    "     ```python\n",
    "     from transformers import ViTModel, ViTConfig\n",
    "\n",
    "     # Creating a ViT model\n",
    "     configuration = ViTConfig()\n",
    "     model = ViTModel(configuration)\n",
    "\n",
    "     # For a pre-trained model\n",
    "     model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "     ```\n",
    "\n",
    "2. Swin Transformer:\n",
    "\n",
    "   - Hugging Face documentation: [Swin Transformer Model](https://huggingface.co/docs/transformers/model_doc/swin)\n",
    "   - Example usage:\n",
    "\n",
    "     ```python\n",
    "     from transformers import SwinModel, SwinConfig\n",
    "\n",
    "     # Creating a Swin Transformer model\n",
    "     configuration = SwinConfig()\n",
    "     model = SwinModel(configuration)\n",
    "\n",
    "     # For a pre-trained model\n",
    "     model = SwinModel.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
    "     ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torchinfo is not installed, install it\n",
    "!pip install torchinfo\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/model_doc/swin#transformers.SwinConfig\n",
    "from transformers import SwinConfig\n",
    "from transformers import SwinForImageClassification\n",
    "\n",
    "swin_config = SwinConfig(\n",
    "    image_size=image_size[0],\n",
    "    num_labels=num_classes,\n",
    "    embed_dim=32,\n",
    "    depths=[2],\n",
    "    num_heads=[4],\n",
    "    window_size=5,\n",
    "    drop_path_rate=0.1,\n",
    ")\n",
    "\n",
    "swin_model = SwinForImageClassification(swin_config)\n",
    "summary(swin_model, input_size=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTConfig\n",
    "from transformers import ViTForImageClassification\n",
    "\n",
    "vit_config = ViTConfig(\n",
    "    image_size=image_size[0],\n",
    "    num_labels=num_classes,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    patch_size=16,\n",
    "    intermediate_size=3072,\n",
    "    hidden_act=\"gelu\",\n",
    "    classifier_dropout=0.1,\n",
    ")\n",
    "\n",
    "vit_model = ViTForImageClassification(vit_config)\n",
    "\n",
    "\n",
    "summary(vit_model, input_size=(1, 3, image_size[0], image_size[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each model (ViT and Swin-ViT), create and submit the best version in terms of performance.\n",
    "\n",
    "Specifically:\n",
    "\n",
    "1. Performance Optimization:\n",
    "\n",
    "   - Experiment with various hyperparameters and configurations for both ViT and Swin-ViT.\n",
    "   - Aim to achieve the highest possible accuracy on the given task (e.g., CIFAR-10 classification).\n",
    "\n",
    "2. Comparative Analysis:\n",
    "\n",
    "   - Provide a short comparison between your ViT and Swin-ViT models.\n",
    "   - Discuss the strengths and weaknesses of each in terms of performance and computational efficiency at **model_checkpoints/assignment2-2-2/report.md**  \n",
    "     !!Tip. Write it briefly. Length and content are not part of the grading score.!!\n",
    "\n",
    "3. Best Model Selection:\n",
    "   - **Submit the best model from either ViT or Swin-ViT.**  \n",
    "     <font color='red'>The scores will be assigned in order based on the highest score, and a perfect score will be given for accuracy of 73% or above</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    optimizer: Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    "):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    pbar = tqdm(loader, desc=\"Training\")\n",
    "\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        logits = outputs.logits\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "        pbar.update(1)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    "):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    pbar = tqdm(loader, desc=\"Evaluating\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in pbar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "            pbar.update(1)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import ViTConfig\n",
    "from transformers import ViTForImageClassification\n",
    "\n",
    "# cpu or gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# or if you have MPS\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10  # DO NOT CHANGE\n",
    "\n",
    "model_name = \"vit\"  # <---- will be used for saving model\n",
    "batch_size = 32  # <---- feel free to change\n",
    "optimizer = optim.Adam  # <---- feel free to change\n",
    "optimizer_kwargs = {  # <---- feel free to change\n",
    "    \"lr\": 0.001,\n",
    "}\n",
    "epochs = 10  # <---- feel free to change\n",
    "seed = 42  # <---- feel free to change\n",
    "\n",
    "# <---- feel free to change\n",
    "model_config = ViTConfig(\n",
    "    image_size=image_size[0],\n",
    "    num_labels=NUM_CLASSES,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=1,\n",
    "    num_attention_heads=1,\n",
    "    patch_size=16,\n",
    "    intermediate_size=3072,\n",
    "    hidden_act=\"gelu\",\n",
    "    classifier_dropout=0.1,\n",
    ")\n",
    "model = ViTForImageClassification(model_config)\n",
    "\n",
    "# or use Swin-ViT\n",
    "#\n",
    "# model_config = SwinConfig(\n",
    "#     patch_size=4,\n",
    "#     image_size=image_size[0],\n",
    "#     num_labels=num_classes,\n",
    "#     embed_dim=32,\n",
    "#     depths=[2],\n",
    "#     num_heads=[4],\n",
    "#     window_size=5,\n",
    "#     drop_path_rate=0.1,\n",
    "# )\n",
    "# model = SwinForImageClassification(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "optimizer = optimizer(model.parameters(), **optimizer_kwargs)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, drop_last=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, drop_last=True\n",
    ")\n",
    "\n",
    "for _ in range(epochs):\n",
    "    result = train(model, train_loader, optimizer, criterion, device)\n",
    "eval_loss, eval_acc = evaluate(model, test_loader, criterion, device)\n",
    "print(f\"Evaluation Loss: {eval_loss:.4f}, Evaluation Accuracy: {eval_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# # ---- save model and config\n",
    "save_path = Path(\"./model_checkpoints/assignment2-2-2\")\n",
    "if not save_path.exists():\n",
    "    save_path.mkdir(parents=True)\n",
    "\n",
    "save_dict = {\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\": epochs,\n",
    "    \"seed\": seed,\n",
    "    \"optimizer\": str(optimizer),\n",
    "    \"optimizer_kwargs\": optimizer_kwargs,\n",
    "    \"test_loss\": eval_loss,\n",
    "    \"test_accuracy\": eval_acc,\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"model_config\": model_config.to_dict(),\n",
    "}\n",
    "\n",
    "model_path = (\n",
    "    save_path\n",
    "    / f\"{model_name}_test_loss_{eval_loss:.4f}_test_accuracy_{eval_acc:.4f}.pth\"\n",
    ")\n",
    "torch.save(save_dict, model_path)\n",
    "print(f\"model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please provide your analysis on each hyper-parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
