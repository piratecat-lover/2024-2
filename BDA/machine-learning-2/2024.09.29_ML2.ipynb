{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDA학회 데이터 분석 모델링반 (ML2) 3주차 복습과제\n",
    "\n",
    "제출자 성명: 이승섭89\n",
    "\n",
    "이전 수업과 같은 내용을 복습했습니다.\n",
    "\n",
    "Python 3.10.14 버전을 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Analysis\n",
    "\n",
    "$$\\underset{\\mathbf{w},b}\\min \\frac{1}{2}||\\mathbf{w}||^2 \\\\ y_{i}(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b) \\geq 1$$\n",
    "\n",
    "- Hard margin SVM\n",
    "    - When data is entirely linearly separable, margin is maximized without any errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\underset{\\mathbf{w},b,\\xi} \\min \\frac{1}{2} ||\\mathbf{w}||^2+C\\sum_{i=1}^{n}\\xi_{i} \\\\ y_{i}(\\mathbf{w} \\cdot \\mathbf{x}_{i} + b) \\geq 1 - \\xi_{i}$$\n",
    "- Soft margin SVM\n",
    "    - When data cannot be entirely separated, margin is maximized with allowance for some wrong classification.\n",
    "    - The constraint is understood as the label of the datapoint being greater or equal to 1-c.\n",
    "    - Why is the constraint condition required?\n",
    "        - Merely maximizing the margin does not guarantee that the data is properly clasisfied.\n",
    "        - The constraint forces datapoints to lie on the correct side of the hyperplane and necessitates balance between margin maximization and classification accuracy.\n",
    "        - y can be either 1 or -1 : the inequality changes for each class: if y = -1, the constraint is\n",
    "        \n",
    "        $$y_{i}(\\mathbf{w} \\cdot \\mathbf{x}_{i} + b) \\geq \\xi_{i} - 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\textbf{Margin}=\\frac{2}{||\\mathbf{w}||}$$\n",
    "\n",
    "- Margin\n",
    "    - The shortest distance between datapoints (support vectors) on the boundary hyperplanes of each class.\n",
    "    - The objective of SVM is to maximize the margin. This is equivalent to minimizing the weight vector $||\\mathbf{w}||$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\underset{\\mathbf{w},b,\\xi} \\min \\left ( \\frac{1}{2}||\\mathbf{w}||^2+C\\sum_{i=1}^{n}\\xi_{i} \\right )$$\n",
    "\n",
    "- Optimization with Lagrange Multiplier Method\n",
    "    - LMM is a method to solve an optimization problem with constraints. For SVM, LMM attempts to minimize the weight vector while satisfying the constraint condition.\n",
    "    - LMM converts the primary problem into a dual problem. This makes the problem easy to transform and simpler to calculate.\n",
    "    - Choosing support vectors: Datapoints greater than zero are chosen as support vectors.\n",
    "    - Kernel tricks: Easier to apply kernel functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculations: \n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{w},b,\\xi,\\alpha,\\mu) = \\frac{1}{2}||\\mathbf{w}||^{2}+C\\sum_{i=1}^{n} \\xi_{i} - \\sum_{i=1}^{n}\\alpha_{i}(y_{i}(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b)-1+\\xi_{i})-\\sum_{i=1}^{n}\\mu_{i}\\xi_{i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\underset{\\alpha}\\max\\sum_{i=1}^{n}\\alpha_{i}-\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\alpha_{i}\\alpha_{j}y_{i}y_{j}(\\mathbf{x}_{i} \\cdot \\mathbf{x}_{j}) \\\\ 0 \\leq \\alpha_{i} \\leq C, \\sum_{i=1}^{n} \\alpha_{i}y_{i}=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{w}=\\sum_{i=1}^{n}\\alpha_{i}y_{i}\\mathbf{x}_{i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$b=y_{k}-\\mathbf{w} \\cdot \\mathbf{x}_{k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Process\n",
    "- Dummy data:\n",
    "    - Positive class: 1\n",
    "    - x1, x2\n",
    "    - x1 = (1,2), y = 1\n",
    "    - x2 = (2,3), y = 1\n",
    "    - Negative class: -1\n",
    "    - x3 = (4,1), y = -1\n",
    "    - x4 = (3,0), y = -1\n",
    "- Create SVM hyperplane and calculate support vectors.\n",
    "- Simple optimization problem: find weight vector and bias with constraint conditions, solve with LMM dual problem.\n",
    "    - Support vector examples: a1 = 0.5, a2 = 0.3, a3 = 0.4, a4 = 0 (not a support vector)\n",
    "- Calculate hyperplane\n",
    "    - $x_1 = (1,2), \\alpha_1 = 0.5, y = 1 \\to (0.5, 1)$\n",
    "    - $x_2 = (2,3), \\alpha_2 = 0.3, y = 1 \\to (0.6, 0.9)$\n",
    "    - $x_3 = (4,1), \\alpha_3 = 0.4, y = -1 \\to (1.6, -0.4)$\n",
    "    - $\\mathbf{w} = (0.5, 1) + (0.6, 0.9) + (-1.6, -0.4) = (-0.5, 1.5)$\n",
    "    - $b = y_1 - \\mathbf{w} \\cdot \\mathbf{x}_1 = -1.5$\n",
    "    - Final equation: $-\\mathbf{w} * \\mathbf{x} + b = 0: -0.5x_1 + 1.5x_2=1.5=0, x_2 = \\frac{1}{3}x_1 + 1$\n",
    "- The support vector values $\\alpha_1 \\sim \\alpha_n$ are optimized with partial derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with Dummy Data: Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "from numpy import linalg\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "np.random.seed(42)\n",
    "X1 = np.random.randn(20,2) + np.array([2,2])\n",
    "X2 = np.random.randn(20,2) + np.array([-2,-2])\n",
    "\n",
    "X = np.vstack([X1, X2])\n",
    "y = np.hstack([np.ones(20), -np.ones(20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data\n",
    "plt.scatter(X[:20,0], X[:20,1], color='b', label='1')\n",
    "plt.scatter(X[20:,0], X[20:,1], color='r', label='-1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "\n",
    "# Linear kernel\n",
    "def linear_kernel(x1, x2):\n",
    "    '''\n",
    "    Calculate the linear kernel between two vectors.\n",
    "    '''\n",
    "    return np.dot(x1, x2)\n",
    "\n",
    "def fit_svm(X, y, C = 1.0, epoch = 1000):\n",
    "    '''\n",
    "    Fit the SVM model. Requires Lagrange multiplier alpha, normal weight vector of hyperplane w, and bias b.\n",
    "    Factors:\n",
    "    X: Training data (n_samples, n_features)\n",
    "    y: Target classes(1 or -1)\n",
    "    C: Penalty term\n",
    "    \n",
    "    Returns:\n",
    "    w: Weight vector\n",
    "    b: Bias\n",
    "    alpha: Optimized Lagrange multiplier\n",
    "    '''\n",
    "    \n",
    "    n_samples, n_features = X.shape\n",
    "    alpha = np.zeros(n_samples) # Initialize alpha\n",
    "    w = np.zeros(n_features) # Initialize weight vector\n",
    "    b = 0 # Initialize bias\n",
    "    \n",
    "    K = np.zeros((n_samples, n_samples)) # Initialize kernel matrix\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_samples):\n",
    "            K[i,j] = linear_kernel(X[i], X[j]) # Calculate kernel matrix (linear kernel)\n",
    "    \n",
    "    # Recursively optimize alpha\n",
    "    for _ in range(epoch):\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                alpha[i] += y[j] * alpha[j] * K[i,j]\n",
    "        alpha = np.maximum(alpha, 0)\n",
    "        \n",
    "    # Calculate weight vector and bias\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        w += alpha[i] * y[i] * X[i]\n",
    "        b += y[i]\n",
    "        b -= np.sum(alpha * y * K[i, :])\n",
    "    b /= n_samples\n",
    "    \n",
    "    return w, b, alpha"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-24-windows",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
